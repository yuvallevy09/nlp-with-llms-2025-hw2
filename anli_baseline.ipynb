{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d027da591b4c458995349de28a43cab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291fafd552da4674b78a4b40ff2c6724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f522cb35b5c74af29d0383526a487b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74fa526c6f4484eafc1304a15c3dd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f83b2fbdaed4df384b2985b769430c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551cb7429ba4405adada2a40b0f12f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da9628fef454c88964f6482e57eafa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7025b06fb1d24e7db10cfb31be840866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888b112c13f94cb4975f32ae15833b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train_r1-00000-of-00001.parqu(‚Ä¶):   0%|          | 0.00/3.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660059c58b9147c58fad65bf5958a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/dev_r1-00000-of-00001.parquet:   0%|          | 0.00/351k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7689583cdf491098b105a3ed9686c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test_r1-00000-of-00001.parque(‚Ä¶):   0%|          | 0.00/353k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f19d069843421faba879ac9fe3b189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train_r2-00000-of-00001.parqu(‚Ä¶):   0%|          | 0.00/6.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1facd8c02150405888c1118c1e183c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/dev_r2-00000-of-00001.parquet:   0%|          | 0.00/351k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b7ee92822149d1a1e19ab154fa4be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test_r2-00000-of-00001.parque(‚Ä¶):   0%|          | 0.00/362k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48df6e6bbb62420eb5f0b7093ac66827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train_r3-00000-of-00001.parqu(‚Ä¶):   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959b6084b7584a85b234094fc126d341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/dev_r3-00000-of-00001.parquet:   0%|          | 0.00/434k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e161c0e549845ae991fad4ade2db852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test_r3-00000-of-00001.parque(‚Ä¶):   0%|          | 0.00/435k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b256f5b975d748ccab5ccdd35caf58ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_r1 split:   0%|          | 0/16946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d05e3ee1e9474094d8c3fbf9d97e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev_r1 split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d57055bebba499bb8bdc1f444736297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_r1 split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68d6467a17545d680783fdf7d7b0843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_r2 split:   0%|          | 0/45460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f45f512705437d9de1eae350a6d81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev_r2 split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f7e5824a4742418cdd157b67718819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_r2 split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b97e5ec41942aebf8b6bce5eb55aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_r3 split:   0%|          | 0/100459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64c0922c6a0401cb05302cff489be78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev_r3 split:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9349c393465641d8bc77bf2923c2cfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_r3 split:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc826baa1734f5db0a5349fafe3366c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bcc3d0e6ba4c58b755714bb5d0b730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c569dd42d1e43d8bfee415749886156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3d577d6b3c469299aaf22a9562cbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f725fe43fb48d288fe6fe61b05f49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e14c341e1a24fca9a0955c70757b8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98d4d9c6c6542aebb5cfff9895dab5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48a23a171354b728c28ca8790a2f057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3e06764b124eb0a0995c3a4a515bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f858feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:39<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) ‚Äî All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) ‚Äî All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright ¬© 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as eval\n",
    "\n",
    "accuracy = eval.load(\"accuracy\")\n",
    "precision = eval.load(\"precision\")\n",
    "recall = eval.load(\"recall\")\n",
    "f1 = eval.load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = eval.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a928179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up evaluation metrics...\n",
      "‚úì Evaluation metrics setup complete\n",
      "\n",
      "============================================================\n",
      "EVALUATING BASELINE MODEL ON ALL TEST SECTIONS\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating test_r1...\n",
      "Dataset size: 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:07<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì test_r1 completed - 1000 examples evaluated\n",
      "  Accuracy: 0.6190\n",
      "  F1: 0.6045\n",
      "  Precision: 0.6332\n",
      "  Recall: 0.6189\n",
      "\n",
      "üìä Evaluating test_r2...\n",
      "Dataset size: 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:42<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì test_r2 completed - 1000 examples evaluated\n",
      "  Accuracy: 0.5040\n",
      "  F1: 0.4893\n",
      "  Precision: 0.5077\n",
      "  Recall: 0.5039\n",
      "\n",
      "üìä Evaluating test_r3...\n",
      "Dataset size: 1200 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:01<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì test_r3 completed - 1200 examples evaluated\n",
      "  Accuracy: 0.4808\n",
      "  F1: 0.4627\n",
      "  Precision: 0.4651\n",
      "  Recall: 0.4817\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF RESULTS\n",
      "============================================================\n",
      "Section    Examples   Accuracy   F1         Precision  Recall    \n",
      "-----------------------------------------------------------------\n",
      "test_r1    1000       0.6190     0.6045     0.6332     0.6189    \n",
      "test_r2    1000       0.5040     0.4893     0.5077     0.5039    \n",
      "test_r3    1200       0.4808     0.4627     0.4651     0.4817    \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ANALYSIS\n",
      "============================================================\n",
      "üìà Total examples: 3200\n",
      "üìà Weighted accuracy: 0.5312\n",
      "\n",
      "üîç Performance by round:\n",
      "   Round 1: Accuracy = 0.6190, F1 = 0.6045\n",
      "   Round 2: Accuracy = 0.5040, F1 = 0.4893\n",
      "   Round 3: Accuracy = 0.4808, F1 = 0.4627\n",
      "\n",
      "üéØ Best: test_r1 (0.6190)\n",
      "‚ö†Ô∏è  Worst: test_r3 (0.4808)\n",
      "\n",
      "üíæ Results saved in 'all_results' variable\n",
      "\n",
      "============================================================\n",
      "TASK 1.1 COMPLETED! ‚úì\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 1.1: evaluation on the ANLI test samples\n",
    "\n",
    "print(\"Setting up evaluation metrics...\")\n",
    "\n",
    "def compute_classification_metrics(predictions_list):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for ANLI predictions using individual metrics\n",
    "    \n",
    "    Args:\n",
    "        predictions_list: List of dictionaries with prediction results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with computed metrics\n",
    "    \"\"\"\n",
    "    # Extract predicted and gold labels\n",
    "    pred_labels = [pred['pred_label'] for pred in predictions_list]\n",
    "    gold_labels = [pred['gold_label'] for pred in predictions_list]\n",
    "    \n",
    "    # Convert string labels to integers\n",
    "    label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    pred_ints = [label_to_int[label] for label in pred_labels]\n",
    "    gold_ints = [label_to_int[label] for label in gold_labels]\n",
    "    \n",
    "    # Load and compute metrics individually (this approach works reliably)\n",
    "    accuracy = eval.load(\"accuracy\")\n",
    "    precision = eval.load(\"precision\")\n",
    "    recall = eval.load(\"recall\")\n",
    "    f1 = eval.load(\"f1\")\n",
    "    \n",
    "    # Compute metrics with macro averaging for multiclass classification\n",
    "    results = {\n",
    "        'accuracy': accuracy.compute(predictions=pred_ints, references=gold_ints)['accuracy'],\n",
    "        'precision': precision.compute(predictions=pred_ints, references=gold_ints, average='macro')['precision'],\n",
    "        'recall': recall.compute(predictions=pred_ints, references=gold_ints, average='macro')['recall'],\n",
    "        'f1': f1.compute(predictions=pred_ints, references=gold_ints, average='macro')['f1']\n",
    "    }\n",
    "    return results\n",
    "\n",
    "print(\"‚úì Evaluation metrics setup complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING BASELINE MODEL ON ALL TEST SECTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on all three test sections\n",
    "test_sections = ['test_r1', 'test_r2', 'test_r3']\n",
    "all_results = {}\n",
    "\n",
    "for section in test_sections:\n",
    "    print(f\"\\nüìä Evaluating {section}...\")\n",
    "    print(f\"Dataset size: {len(dataset[section])} examples\")\n",
    "    \n",
    "    # Run evaluation on this section\n",
    "    predictions = evaluate_on_dataset(dataset[section])\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_classification_metrics(predictions)\n",
    "    all_results[section] = {\n",
    "        'metrics': metrics,\n",
    "        'predictions': predictions,\n",
    "        'num_examples': len(predictions)\n",
    "    }\n",
    "    \n",
    "    # Print results for this section\n",
    "    print(f\"‚úì {section} completed - {len(predictions)} examples evaluated\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary table\n",
    "print(f\"{'Section':<10} {'Examples':<10} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for section in test_sections:\n",
    "    metrics = all_results[section]['metrics']\n",
    "    num_examples = all_results[section]['num_examples']\n",
    "    print(f\"{section:<10} {num_examples:<10} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall statistics\n",
    "total_examples = sum(all_results[section]['num_examples'] for section in test_sections)\n",
    "weighted_accuracy = sum(all_results[section]['metrics']['accuracy'] * all_results[section]['num_examples'] \n",
    "                       for section in test_sections) / total_examples\n",
    "\n",
    "print(f\"üìà Total examples: {total_examples}\")\n",
    "print(f\"üìà Weighted accuracy: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# Performance across rounds\n",
    "print(f\"\\nüîç Performance by round:\")\n",
    "for i, section in enumerate(test_sections, 1):\n",
    "    metrics = all_results[section]['metrics']\n",
    "    print(f\"   Round {i}: Accuracy = {metrics['accuracy']:.4f}, F1 = {metrics['f1']:.4f}\")\n",
    "\n",
    "# Best/worst sections\n",
    "accuracies = {section: all_results[section]['metrics']['accuracy'] for section in test_sections}\n",
    "best_section = max(accuracies, key=accuracies.get)\n",
    "worst_section = min(accuracies, key=accuracies.get)\n",
    "\n",
    "print(f\"\\nüéØ Best: {best_section} ({accuracies[best_section]:.4f})\")\n",
    "print(f\"‚ö†Ô∏è  Worst: {worst_section} ({accuracies[worst_section]:.4f})\")\n",
    "\n",
    "print(f\"\\nüíæ Results saved in 'all_results' variable\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1.1 COMPLETED! ‚úì\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f2b6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TASK 1.2: SAMPLING ERRORS FOR ANALYSIS\n",
      "============================================================\n",
      "üìä Total errors found: 1500\n",
      "   test_r1: 381 errors\n",
      "   test_r2: 496 errors\n",
      "   test_r3: 623 errors\n",
      "\n",
      "üîç Here are 20 randomly sampled errors for investigation:\n",
      "================================================================================\n",
      "\n",
      "--- ERROR 1 ---\n",
      "Section: test_r3\n",
      "Premise: A missed call is a telephone call that is deliberately terminated by the caller before being answered by its intended recipient, in order to communicate a pre-agreed message without paying the cost of a call. For example, a group of friends may agree that two missed calls in succession means \"I am running late\". The practice is common in South Asia, the Philippines and Africa.\n",
      "Hypothesis: Pre-agreed missed call messages are only practiced in 3 countries.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 1.3, 'neutral': 96.5, 'contradiction': 2.1}\n",
      "Human Reason: The context  does specify if the countries mentioned are the only ones using this pre-message system.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 2 ---\n",
      "Section: test_r1\n",
      "Premise: John-Michael Hakim Gibson, (born August 15, 1990), better known by his stage name Cash Out (stylized Ca$h Out) is an American rapper originally from Columbus, Georgia, and later raised in Atlanta, Georgia. His debut album \"Let's Get It\", was released on August 26, 2014 and was preceded by the lead single \"She Twerkin\".\n",
      "Hypothesis: Gibson was 18 years old when he released his first albem\n",
      "Model Predicted: entailment\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 68.6, 'neutral': 4.5, 'contradiction': 26.9}\n",
      "Human Reason: He was born in 1990 and released his debut album in 2014, making him about 24 years old\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 3 ---\n",
      "Section: test_r1\n",
      "Premise: Svein Holden (born 23 August 1973) is a Norwegian jurist having prosecuted several major criminal cases in Norway. Together with prosecutor Inga Bejer Engh Holden prosecuted terror suspect Anders Behring Breivik in the 2012 trial following the 2011 Norway attacks.\n",
      "Hypothesis: Svein Holden is 45 years old.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 1.3, 'neutral': 0.2, 'contradiction': 98.5}\n",
      "Human Reason: he was born on august 23 1973, thus as of 8/11/2019 he is 45. i think it was difficult because the initial phrase does not say his specific age.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 4 ---\n",
      "Section: test_r2\n",
      "Premise: \"Vanlose Stairway\" is a song written by Northern Irish singer-songwriter Van Morrison and included on his 1982 album, \"Beautiful Vision\". It has remained a popular concert performance throughout Morrison's career and has become one of his most played songs.\n",
      "Hypothesis: Vanlose Stairway is a Van Morrison Song and on an abum\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 1.5, 'neutral': 1.5, 'contradiction': 97.0}\n",
      "Human Reason: Its just vague enough to cause it problems\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 5 ---\n",
      "Section: test_r2\n",
      "Premise: Peeya Rai Chowdhary is an Indian actress. Peeya Rai was married to model Shayan Munshi in 2006, but separated from him in 2010. She played Lakhi in Gurinder Chadha's \"Bride and Prejudice,\" Rita in the movie \"The Bong Connection\" (where she worked with husband Munshi) and played \"Kiran\" in the TV show \"Hip Hip Hurray\". She studied at National College, Mumbai.\n",
      "Hypothesis: Peeya Rai was not married to Munshi while she was in the TV show Hip Hip Hurray.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 89.1, 'neutral': 2.6, 'contradiction': 8.3}\n",
      "Human Reason: The context doesn't state what year the TV show was made so we don't know whether she was married or not.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 6 ---\n",
      "Section: test_r2\n",
      "Premise: Flatbush Avenue is a major avenue in the New York City Borough of Brooklyn. It runs from the Manhattan Bridge south-southeastward to Jamaica Bay, where it joins the Marine Parkway‚ÄìGil Hodges Memorial Bridge, which connects Brooklyn to the Rockaway Peninsula in Queens. The north end was extended to the Manhattan Bridge as \"Flatbush Avenue Extension.\"\n",
      "Hypothesis: The north end extension was going to be called \"Flatbush Avenue Extension,\" Pt. 2, but wasn't.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 1.3, 'neutral': 69.6, 'contradiction': 29.1}\n",
      "Human Reason: There's no indication whether it was ever going to be called anything else.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 7 ---\n",
      "Section: test_r1\n",
      "Premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as \"South Kal Mines - New Celebration\", being a merger of the former \"New Celebration Gold Mine\" and the \"Jubilee Gold Mine\", which were combined in 2002.\n",
      "Hypothesis: The mine should be called South West Kalgoorie Mine\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.2, 'neutral': 2.0, 'contradiction': 97.8}\n",
      "Human Reason: My statement was a matter of opinion based off of an objective fact.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 8 ---\n",
      "Section: test_r1\n",
      "Premise: The Robinson R44 is a four-seat light helicopter produced by Robinson Helicopter Company since 1992. Based on the company's two-seat Robinson R22, the R44 features hydraulically assisted flight controls. It was first flown on 31 March 1990 and received FAA certification in December 1992, with the first delivery in February 1993.\n",
      "Hypothesis: It took three years for the Robinson R44 to receive certification from the FAA from the time it was first flown until the time certification was granted.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 90.5, 'neutral': 0.5, 'contradiction': 8.9}\n",
      "Human Reason: It took only two years for the certification to be granted.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 9 ---\n",
      "Section: test_r3\n",
      "Premise: The exchanges resulted in greatly improved financial, oil, fisheries, and military issues, but Great Britain consistently refused to address the issue of sovereignty over the Falklands.\n",
      "Hypothesis: Great Britain refused to address the latest Issue of the financial publication sovereignty to the falklands\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 3.8, 'neutral': 45.6, 'contradiction': 50.6}\n",
      "Human Reason: The created statement says that there is a publication called sovereignty which clearly isn't true however it also says that Great Britain didn't address this to the Falklands which is correct.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 10 ---\n",
      "Section: test_r3\n",
      "Premise: How to get a bank account<br>Choose a banking institution. Perhaps the most important step in opening an account is deciding which bank to do business with. Compare several banks in your area using criteria that are important to you, such as the branch hours and the availability of atms.\n",
      "Hypothesis: You can only open it in a branch\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 1.9, 'neutral': 91.6, 'contradiction': 6.5}\n",
      "Human Reason: It does not really say whether or not you can open one in a branch, so it is neither correct or incorrect.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 11 ---\n",
      "Section: test_r1\n",
      "Premise: The Nordic Resistance Movement (Swedish: \"Nordiska Motst√•ndsr√∂relsen; NMR\" , Norwegian: \"Nordiske motstandsbevegelsen; NMB\" , Finnish: \"Pohjoismainen vastarintaliike; PVL\" , Danish: \"Nordiske modstandsbev√¶gelse; NMB\" ) is a Nordic Neo-Nazi movement that exists in Sweden, Finland, and Norway. It had a branch in Denmark before it was disbanded for inactivity in 2016.\n",
      "Hypothesis: The Nordic Resistance Movement started in 2015.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.2, 'neutral': 97.5, 'contradiction': 2.3}\n",
      "Human Reason: It is unclear when the Nordic Resistance Movement was started. The system did not understand the context of the statement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 12 ---\n",
      "Section: test_r3\n",
      "Premise: In this regard, the Bloc Quebecois would have preferred that the Export Development Corporation draw more on the very simple and probably more effective operational framework of the World Bank or the European Bank for Reconstruction and Development, since they require, for each sensitive project in a sensitive area, an impact study, public hearings and most importantly process transparency.\n",
      "Hypothesis: Bloc Quebecois will no longer express his preferences when the World Bank is involved.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.1, 'neutral': 99.1, 'contradiction': 0.8}\n",
      "Human Reason: We have no clue whether Bloc will or will not express his preferences when the World Bank is involved. Not sure how this fooled the AI system.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 13 ---\n",
      "Section: test_r2\n",
      "Premise: Along the Shadow is the third studio album by American rock band Saosin, released on May 20, 2016 through Epitaph Records. The album marks the end of a three-and-a-half-year hiatus for the group with the return of original lead vocalist Anthony Green. It also marks the subsequent departure of lead guitarist Justin Shekoski.\n",
      "Hypothesis: Anthony Green is a licensed funeral director. \n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.1, 'neutral': 41.0, 'contradiction': 59.0}\n",
      "Human Reason: The text does not mention Anthony Green being a funeral director, but it is possible that he is. \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 14 ---\n",
      "Section: test_r1\n",
      "Premise: The 1938 Duke Blue Devils football team represented the Duke Blue Devils of Duke University during the 1938 college football season. They were led by head coach Wallace Wade, who was in his eight season at the school. Known as the \"Iron Dukes,\" the 1938 Blue Devils went undefeated and unscored upon during the entire regular season, earning them the Southern Conference championship.\n",
      "Hypothesis: Wallace Wade started at Duke in 1930.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 13.9, 'neutral': 37.1, 'contradiction': 49.0}\n",
      "Human Reason: Wallace Wade was in his 8th year at Duke in 1938, so he must have signed on in 1930\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 15 ---\n",
      "Section: test_r1\n",
      "Premise: Jaana Kunitz, born in Taivalkoski, Finland in 1972, is a professional dance instructor based in San Diego, California. She is married to her dance partner, James Kunitz, and has since retired from competing in ballroom dance competitions to focus on coaching, video production, and dance-fitness programs.\n",
      "Hypothesis: James Kunitz met Jaana in Atlanta \n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.0, 'neutral': 98.6, 'contradiction': 1.3}\n",
      "Human Reason: No way to know if they met in Atlanta or not.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 16 ---\n",
      "Section: test_r1\n",
      "Premise: The Alternative Press Tour or AP Tour was an American/Canadian concert tour that began in 2007 by the magazine company \"Alternative Press\". It featured diverse bands like Black Veil Brides, All Time Low, Bring Me The Horizon, Cute Is What We Aim For, Never Shout Never, and 3OH!3. The tour was announced in the April or November issues of Alternative Press.\n",
      "Hypothesis: The Alternative Press Tour no longer exists.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 94.4, 'neutral': 5.5, 'contradiction': 0.1}\n",
      "Human Reason: Its not clear if the event is still happening.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 17 ---\n",
      "Section: test_r2\n",
      "Premise: Alice Sue Claeys (born February 24, 1975) is a former competitive figure skater. Representing Belgium, she won silver at the 1992 Skate Canada International and finished in the top ten at three ISU Championships ‚Äî the 1992 World Junior Championships (4th), the 1992 World Championships (7th), and the 1993 European Championships (8th).\n",
      "Hypothesis: Alice Sue Claeys continued to play professional figure skating past 1993.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.3, 'neutral': 98.3, 'contradiction': 1.4}\n",
      "Human Reason: While Claeys was active as a professional figure skating in 1993, it's unsure if she was, or was not, beyond 1993. Due to the fact that the text stated she was in 1993, and how I worded my sentence and including this year, it confused the agent.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 18 ---\n",
      "Section: test_r2\n",
      "Premise: Mutual Friends is a British comedy drama television series broadcast in six episodes on BBC One in from 26 August until 30 September 2008. The series starred Marc Warren, Alexander Armstrong, Keeley Hawes, Sarah Alexander, Claire Rushbrook, Emily Joyce, Naomi Bentley and Joshua Sarphie as a group of old friends whose lives are thrown into chaos when one of their group commits suicide.\n",
      "Hypothesis: Mutual Friends had 8 protagonists\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 23.2, 'neutral': 10.3, 'contradiction': 66.5}\n",
      "Human Reason: The text names 8 protagonists\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 19 ---\n",
      "Section: test_r3\n",
      "Premise: Blue<br>We have a cat named Blue. We found him as a small kitten. He has very large ears. Even small noises make him jump! He is the biggest scaredy-cat I have ever seen!\n",
      "Hypothesis: Blue is still a kitten.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 59.3, 'neutral': 21.6, 'contradiction': 19.1}\n",
      "Human Reason: We don't know for sure if Blue has grown up to be a cat now or if he's still a kitten.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 20 ---\n",
      "Section: test_r3\n",
      "Premise: The petitioners are drawing attention to the fact that our region has been heavily affected by the groundfish moratorium imposed by Fisheries and Oceans Canada back in May 1994. Since then, the Atlantic Groundfish Strategy, or TAGS, has been the only means of survival for a large part of our population.\n",
      "Hypothesis: Survival of the tinny population\n",
      "Model Predicted: entailment\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 87.3, 'neutral': 8.0, 'contradiction': 4.7}\n",
      "Human Reason: From 1994 Atlantic Groundfish Strategy, or TAGS means of survival for a large part of our population.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ 20 error samples displayed!\n",
      "üìù Copy these examples and send them for detailed analysis.\n"
     ]
    }
   ],
   "source": [
    "# Task 1.2: Sample 20 Errors for Investigation\n",
    "\n",
    "import random\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TASK 1.2: SAMPLING ERRORS FOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all errors from all test sections\n",
    "all_errors = []\n",
    "\n",
    "for section_name, section_data in all_results.items():\n",
    "    predictions = section_data['predictions']\n",
    "    for pred in predictions:\n",
    "        if pred['pred_label'] != pred['gold_label']:  # This is an error\n",
    "            error_info = {\n",
    "                'section': section_name,\n",
    "                'premise': pred['premise'],\n",
    "                'hypothesis': pred['hypothesis'],\n",
    "                'predicted': pred['pred_label'],\n",
    "                'gold': pred['gold_label'],\n",
    "                'reason': pred['reason'],\n",
    "                'prediction_scores': pred['prediction']\n",
    "            }\n",
    "            all_errors.append(error_info)\n",
    "\n",
    "print(f\"üìä Total errors found: {len(all_errors)}\")\n",
    "for section in ['test_r1', 'test_r2', 'test_r3']:\n",
    "    section_errors = [e for e in all_errors if e['section'] == section]\n",
    "    print(f\"   {section}: {len(section_errors)} errors\")\n",
    "\n",
    "# Sample 20 errors for analysis\n",
    "random.seed(42)  # For reproducible results\n",
    "sampled_errors = random.sample(all_errors, min(20, len(all_errors)))\n",
    "\n",
    "print(f\"\\nüîç Here are {len(sampled_errors)} randomly sampled errors for investigation:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display each error clearly\n",
    "for i, error in enumerate(sampled_errors, 1):\n",
    "    print(f\"\\n--- ERROR {i} ---\")\n",
    "    print(f\"Section: {error['section']}\")\n",
    "    print(f\"Premise: {error['premise']}\")\n",
    "    print(f\"Hypothesis: {error['hypothesis']}\")\n",
    "    print(f\"Model Predicted: {error['predicted']}\")\n",
    "    print(f\"Correct Answer: {error['gold']}\")\n",
    "    print(f\"Prediction Confidence: {error['prediction_scores']}\")\n",
    "    print(f\"Human Reason: {error['reason']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n‚úÖ {len(sampled_errors)} error samples displayed!\")\n",
    "print(\"üìù Copy these examples and send them for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8b058",
   "metadata": {},
   "source": [
    "### Task 1.2: Reasons the model made a mistake  \n",
    "\n",
    "\n",
    "#### 1. **Mathematical/Numerical Reasoning Failures** (25%)\n",
    "**Errors: 2, 3, 8, 14, 18**\n",
    "\n",
    "- **Error 2**: Failed age calculation (1990‚Üí2014 = 24 years, not 18)\n",
    "- **Error 3**: Failed age calculation (1973‚Üí2019 = 45-46 years) \n",
    "- **Error 8**: Failed duration calculation (March 1990‚ÜíDec 1992 ‚âà 2.75 years, not 3)\n",
    "- **Error 14**: Failed reverse calculation (8th year in 1938 ‚Üí started 1930)\n",
    "- **Error 18**: Failed counting (8 named actors = 8 protagonists)\n",
    "\n",
    "**Pattern**: Model struggles with basic arithmetic, date calculations, and counting tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Missing Information ‚Üí False Contradiction** (40%)\n",
    "**Errors: 1, 6, 10, 11, 12, 13, 15, 17**\n",
    "\n",
    "- **Error 1**: \"Only 3 countries\" ‚Üí Model sees contradiction instead of neutral (no exclusivity stated)\n",
    "- **Error 6**: Extension name ‚Üí Model assumes contradiction from missing info\n",
    "- **Error 10**: \"Only in branch\" ‚Üí Model contradicts unstated restriction  \n",
    "- **Error 11**: Start date claim ‚Üí Model treats missing date as contradiction\n",
    "- **Error 12**: Future preferences ‚Üí Model assumes contradiction from unknown future\n",
    "- **Error 13**: Profession claim ‚Üí Model contradicts unstated profession\n",
    "- **Error 15**: Meeting location ‚Üí Model contradicts unstated location\n",
    "- **Error 17**: Career continuation ‚Üí Model contradicts missing timeline info\n",
    "\n",
    "**Pattern**: Model is overly aggressive in predicting contradictions when information is simply absent. Should predict \"neutral\" but defaults to \"contradiction.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Temporal Reasoning Errors** (15%)\n",
    "**Errors: 5, 16, 19**\n",
    "\n",
    "- **Error 5**: Marriage timeline vs TV show timing ‚Üí Can't handle missing temporal overlap\n",
    "- **Error 16**: Past tense description ‚Üí Incorrectly infers current non-existence  \n",
    "- **Error 19**: \"Found as kitten\" ‚Üí Incorrectly assumes current state\n",
    "\n",
    "**Pattern**: Model struggles with temporal states, timeline inference, and distinguishing past vs present states.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Basic Reading Comprehension** (10%)\n",
    "**Errors: 4, 20**\n",
    "\n",
    "- **Error 4**: Clear text states \"Van Morrison song on album\" ‚Üí Model predicts contradiction\n",
    "- **Error 20**: Text confusion between \"large part\" vs \"tinny population\"\n",
    "\n",
    "**Pattern**: Fundamental misreading of clear, direct statements.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Complex Linguistic Reasoning** (5%)\n",
    "**Error: 9**\n",
    "\n",
    "- **Error 9**: Wordplay with \"sovereignty\" as publication name ‚Üí Model missed that core claim (Britain refused to address sovereignty) remains true despite confusing wording\n",
    "\n",
    "**Pattern**: Difficulty with complex sentence structures and embedded meaning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Opinion vs Fact Confusion** (5%) \n",
    "**Error: 7**\n",
    "\n",
    "- **Error 7**: \"Should be called\" (opinion) ‚Üí Model treats normative statement as factual contradiction\n",
    "\n",
    "**Pattern**: Cannot distinguish between factual claims and opinion statements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - hw2)",
   "language": "python",
   "name": "hw2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
