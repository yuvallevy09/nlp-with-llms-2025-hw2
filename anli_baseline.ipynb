{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"] and pred_dict[\"contradiction\"] > pred_dict[\"neutral\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f858feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [03:06<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate as eval\n",
    "\n",
    "accuracy = eval.load(\"accuracy\")\n",
    "precision = eval.load(\"precision\")\n",
    "recall = eval.load(\"recall\")\n",
    "f1 = eval.load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = eval.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a928179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up evaluation metrics...\n",
      "✓ Evaluation metrics setup complete\n",
      "\n",
      "============================================================\n",
      "EVALUATING BASELINE MODEL ON ALL TEST SECTIONS\n",
      "============================================================\n",
      "\n",
      "📊 Evaluating test_r1...\n",
      "Dataset size: 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:30<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ test_r1 completed - 1000 examples evaluated\n",
      "  Accuracy: 0.7120\n",
      "  F1: 0.7119\n",
      "  Precision: 0.7135\n",
      "  Recall: 0.7120\n",
      "\n",
      "📊 Evaluating test_r2...\n",
      "Dataset size: 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:25<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ test_r2 completed - 1000 examples evaluated\n",
      "  Accuracy: 0.5470\n",
      "  F1: 0.5465\n",
      "  Precision: 0.5472\n",
      "  Recall: 0.5470\n",
      "\n",
      "📊 Evaluating test_r3...\n",
      "Dataset size: 1200 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:58<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ test_r3 completed - 1200 examples evaluated\n",
      "  Accuracy: 0.4950\n",
      "  F1: 0.4943\n",
      "  Precision: 0.4985\n",
      "  Recall: 0.4946\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF RESULTS\n",
      "============================================================\n",
      "Section    Examples   Accuracy   F1         Precision  Recall    \n",
      "-----------------------------------------------------------------\n",
      "test_r1    1000       0.7120     0.7119     0.7135     0.7120    \n",
      "test_r2    1000       0.5470     0.5465     0.5472     0.5470    \n",
      "test_r3    1200       0.4950     0.4943     0.4985     0.4946    \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ANALYSIS\n",
      "============================================================\n",
      "📈 Total examples: 3200\n",
      "📈 Weighted accuracy: 0.5791\n",
      "\n",
      "🔍 Performance by round:\n",
      "   Round 1: Accuracy = 0.7120, F1 = 0.7119\n",
      "   Round 2: Accuracy = 0.5470, F1 = 0.5465\n",
      "   Round 3: Accuracy = 0.4950, F1 = 0.4943\n",
      "\n",
      "🎯 Best: test_r1 (0.7120)\n",
      "⚠️  Worst: test_r3 (0.4950)\n",
      "\n",
      "💾 Results saved in 'all_results' variable\n",
      "\n",
      "============================================================\n",
      "TASK 1.1 COMPLETED! ✓\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 1.1: evaluation on the ANLI test samples\n",
    "\n",
    "print(\"Setting up evaluation metrics...\")\n",
    "\n",
    "def compute_classification_metrics(predictions_list):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for ANLI predictions using individual metrics\n",
    "    \n",
    "    Args:\n",
    "        predictions_list: List of dictionaries with prediction results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with computed metrics\n",
    "    \"\"\"\n",
    "    # Extract predicted and gold labels\n",
    "    pred_labels = [pred['pred_label'] for pred in predictions_list]\n",
    "    gold_labels = [pred['gold_label'] for pred in predictions_list]\n",
    "    \n",
    "    # Convert string labels to integers\n",
    "    label_to_int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    pred_ints = [label_to_int[label] for label in pred_labels]\n",
    "    gold_ints = [label_to_int[label] for label in gold_labels]\n",
    "    \n",
    "    # Load and compute metrics individually (this approach works reliably)\n",
    "    accuracy = eval.load(\"accuracy\")\n",
    "    precision = eval.load(\"precision\")\n",
    "    recall = eval.load(\"recall\")\n",
    "    f1 = eval.load(\"f1\")\n",
    "    \n",
    "    # Compute metrics with macro averaging for multiclass classification\n",
    "    results = {\n",
    "        'accuracy': accuracy.compute(predictions=pred_ints, references=gold_ints)['accuracy'],\n",
    "        'precision': precision.compute(predictions=pred_ints, references=gold_ints, average='macro')['precision'],\n",
    "        'recall': recall.compute(predictions=pred_ints, references=gold_ints, average='macro')['recall'],\n",
    "        'f1': f1.compute(predictions=pred_ints, references=gold_ints, average='macro')['f1']\n",
    "    }\n",
    "    return results\n",
    "\n",
    "print(\"✓ Evaluation metrics setup complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING BASELINE MODEL ON ALL TEST SECTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on all three test sections\n",
    "test_sections = ['test_r1', 'test_r2', 'test_r3']\n",
    "all_results = {}\n",
    "\n",
    "for section in test_sections:\n",
    "    print(f\"\\n📊 Evaluating {section}...\")\n",
    "    print(f\"Dataset size: {len(dataset[section])} examples\")\n",
    "    \n",
    "    # Run evaluation on this section\n",
    "    predictions = evaluate_on_dataset(dataset[section])\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_classification_metrics(predictions)\n",
    "    all_results[section] = {\n",
    "        'metrics': metrics,\n",
    "        'predictions': predictions,\n",
    "        'num_examples': len(predictions)\n",
    "    }\n",
    "    \n",
    "    # Print results for this section\n",
    "    print(f\"✓ {section} completed - {len(predictions)} examples evaluated\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create summary table\n",
    "print(f\"{'Section':<10} {'Examples':<10} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for section in test_sections:\n",
    "    metrics = all_results[section]['metrics']\n",
    "    num_examples = all_results[section]['num_examples']\n",
    "    print(f\"{section:<10} {num_examples:<10} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall statistics\n",
    "total_examples = sum(all_results[section]['num_examples'] for section in test_sections)\n",
    "weighted_accuracy = sum(all_results[section]['metrics']['accuracy'] * all_results[section]['num_examples'] \n",
    "                       for section in test_sections) / total_examples\n",
    "\n",
    "print(f\"📈 Total examples: {total_examples}\")\n",
    "print(f\"📈 Weighted accuracy: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# Performance across rounds\n",
    "print(f\"\\n🔍 Performance by round:\")\n",
    "for i, section in enumerate(test_sections, 1):\n",
    "    metrics = all_results[section]['metrics']\n",
    "    print(f\"   Round {i}: Accuracy = {metrics['accuracy']:.4f}, F1 = {metrics['f1']:.4f}\")\n",
    "\n",
    "# Best/worst sections\n",
    "accuracies = {section: all_results[section]['metrics']['accuracy'] for section in test_sections}\n",
    "best_section = max(accuracies, key=accuracies.get)\n",
    "worst_section = min(accuracies, key=accuracies.get)\n",
    "\n",
    "print(f\"\\n🎯 Best: {best_section} ({accuracies[best_section]:.4f})\")\n",
    "print(f\"⚠️  Worst: {worst_section} ({accuracies[worst_section]:.4f})\")\n",
    "\n",
    "print(f\"\\n💾 Results saved in 'all_results' variable\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1.1 COMPLETED! ✓\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f2b6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TASK 1.2: SAMPLING ERRORS FOR ANALYSIS\n",
      "============================================================\n",
      "📊 Total errors found: 1347\n",
      "   test_r1: 288 errors\n",
      "   test_r2: 453 errors\n",
      "   test_r3: 606 errors\n",
      "\n",
      "🔍 Here are 20 randomly sampled errors for investigation:\n",
      "================================================================================\n",
      "\n",
      "--- ERROR 1 ---\n",
      "Section: test_r3\n",
      "Premise: The National Park Trust identified 20 high-priority sites - including the Blue Ridge Parkway in North Carolina and Virginia and Everglades National Park in Florida - as areas with private property that could be sold.\n",
      "Hypothesis: There are areas with property that could be sold in many states\n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 12.7, 'neutral': 86.9, 'contradiction': 0.4}\n",
      "Human Reason: There were at least 20 states, because the 20 is only considered high priority, there are probably more, which means the word \"many\" is probably applicable\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 2 ---\n",
      "Section: test_r1\n",
      "Premise: Gustave Marie Maurice Mesny (28 March 1886 – 19 January 1945) was a French Army general in command of the 5th North African Infantry Division who was captured during the Second World War. Mesny was unlawfully executed in retribution for the death of German general Fritz von Brodowski.\n",
      "Hypothesis: Gustave was a Brigadier General. \n",
      "Model Predicted: neutral\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 2.2, 'neutral': 91.8, 'contradiction': 5.9}\n",
      "Human Reason: Gustave Mensy was a General. Completely separate from a Brigadier General.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 3 ---\n",
      "Section: test_r1\n",
      "Premise: The Face of Medusa (Greek: Το πρόσωπο της Μέδουσας , translit. To prosopo tis Medousas and also known as \"Vortex\") is a 1967 Greek drama film directed by Nikos Koundouros. It was entered into the 17th Berlin International Film Festival. It features a beautiful man-eating woman on a remote Greek island, eating stranded men.\n",
      "Hypothesis: The Face of Medusa was filmed in Europe\n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 22.2, 'neutral': 77.3, 'contradiction': 0.4}\n",
      "Human Reason: It is a Greek film\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 4 ---\n",
      "Section: test_r2\n",
      "Premise: Johns Creek is a city located in Fulton County in the U.S. state of Georgia. According to the 2010 U.S. Census, the population was 76,728. The city is an affluent northeastern suburb of Atlanta. In 2017 Johns Creek ranked third on the \"USA TODAY\" list of \"50 best cities to live in.\"\n",
      "Hypothesis: Johns Creek has a population of 92,000.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.0, 'neutral': 0.3, 'contradiction': 99.7}\n",
      "Human Reason: The information from the census said here is 9 years old, so we do not know the population now. The system is taking the 2010 data as current.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 5 ---\n",
      "Section: test_r2\n",
      "Premise: Patricia Donoho Hughes (August 18, 1930 – January 20, 2010) was a First Lady of Maryland, married to former Maryland Governor Harry Hughes. She was educated at Sorbonne (1949) and Bryn Mawr College (1951) before getting married on June 30, 1951. She later continued her education at the University of Delaware (1966). Mrs. Hughes was a teacher and educator by profession.\n",
      "Hypothesis: Particia Donoho Hughes was the First Lady of Maryland in 2010.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 6.8, 'neutral': 10.8, 'contradiction': 82.5}\n",
      "Human Reason: Patricia Donoho Hughes died in 2010 but it's unclear whether she was still the First Lady upon her death. It's possible she still was but it's not certain. The algorithm likely got confused by the reference to whether or not she was First Lady.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 6 ---\n",
      "Section: test_r2\n",
      "Premise: The Local Government (Northern Ireland) Act 1972 (1972 c. 9) was an Act of the Parliament of Northern Ireland that constituted district councils to administer the twenty-six local government districts created by the Local Government (Boundaries) Act (Northern Ireland) 1971, and abolished the existing local authorities in Northern Ireland.\n",
      "Hypothesis: Local authority was removed in favor of localized governance \n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 36.4, 'neutral': 63.3, 'contradiction': 0.3}\n",
      "Human Reason: The LGA Act abolished local authority and replaced with 26 local government districts. The model probably couldn't tell what 'governance' referred to\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 7 ---\n",
      "Section: test_r1\n",
      "Premise: Choi Min-sik ( ] ; born January 22, 1962) is a South Korean actor. He is best known for his critically acclaimed roles in \"Oldboy\" (2003), \"I Saw the Devil\" (2010), and \"\" (2014). He also starred alongside Scarlett Johansson in the 2014 French film \"Lucy\".\n",
      "Hypothesis: Choi Min-sik has been an actor for at least 10 years.\n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 7.9, 'neutral': 91.7, 'contradiction': 0.3}\n",
      "Human Reason: My statement is correct because Choi Min-sik had movies in 2003 and 2014, meaning his acting career was definitely at least 10 years. The system was probably fooled because I used a number not present in the article.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 8 ---\n",
      "Section: test_r1\n",
      "Premise: The Face of Medusa (Greek: Το πρόσωπο της Μέδουσας , translit. To prosopo tis Medousas and also known as \"Vortex\") is a 1967 Greek drama film directed by Nikos Koundouros. It was entered into the 17th Berlin International Film Festival. It features a beautiful man-eating woman on a remote Greek island, eating stranded men.\n",
      "Hypothesis: The Face of Medusa is a 1967 Greek drama film that starred by Nikos Koundouros.\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 0.2, 'neutral': 3.2, 'contradiction': 96.7}\n",
      "Human Reason: Its not known who starred in the film. The system is confused when a person that is in the text is given to do something different than what is described in the text.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 9 ---\n",
      "Section: test_r3\n",
      "Premise: For those who have just now picked up the debate, we are dealing with the fact the Reform Party thinks that we should target any future surpluses toward paying down the debt and offering tax relief, and chastising the government for its plans to spend 50% of any new spending or any budgetary surplus on new spending.\n",
      "Hypothesis: the intended audience is long term followers that the speaker is addressing\n",
      "Model Predicted: neutral\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 0.0, 'neutral': 99.8, 'contradiction': 0.2}\n",
      "Human Reason: The speaker is addressing new followers as the phrase \"for those who have just now picked up the debate ' implies. This makes my statement incorrect. The model was probably thrown off by the way I worded my sentence. I phrased my statement in a way that might fool the model's method of evaluating statements.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 10 ---\n",
      "Section: test_r1\n",
      "Premise: Sir is an honorific address used in a number of situations in many anglophone cultures. The term can be used as a formal prefix, especially in the Commonwealth, for males who have been given certain honours or titles (such as knights and baronets), where usage is strictly governed by law and custom.\n",
      "Hypothesis: Baronets were called sir in their day.\n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 1.9, 'neutral': 97.4, 'contradiction': 0.8}\n",
      "Human Reason: The prompt shows that people who were knights or baronets were called sir.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 11 ---\n",
      "Section: test_r3\n",
      "Premise: The 2017 EFL League Two play-off Final was a football match that was contested between Blackpool and Exeter City. The match was played at Wembley Stadium on 28 May 2017. Blackpool won the game 2-1 and were promoted to League One for the 2017-18 season.\n",
      "Hypothesis: The promotion lasted a year\n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 0.0, 'neutral': 99.9, 'contradiction': 0.0}\n",
      "Human Reason: The promotion lasted for the 2017-18 year which is one year making my statement correct. I am guessing the model got confused by the way I phrased the dates\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 12 ---\n",
      "Section: test_r3\n",
      "Premise: The Walkie Talkie<br>The boys loved playing outside. They had walkie talkies that the used to talk to each other. They would run through the woods and keep in contact with them. They played this game all summer. THey couldn't wait until next summer to play it again!\n",
      "Hypothesis: The walkie talkies made it easy for the boys to keep in contact with each other \n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 3.8, 'neutral': 96.2, 'contradiction': 0.1}\n",
      "Human Reason: The whole context is about how the boys used the walkie talkies to talk to each other the whole summer. I think the system didn't guess right because I didn't use a lot of the same words from the context in my answer.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 13 ---\n",
      "Section: test_r1\n",
      "Premise: A scrum machine, or scrummaging machine, is a padded, weighty device against which a pack of rugby football forwards can practice scrummaging and rucking. The purpose of the scrum machine is to provide teams with a safe tool with which to improve the strength and skills of their players.\n",
      "Hypothesis: At least one specific type of machine was successfully developed to make practicing rugby safer. \n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 0.5, 'neutral': 99.4, 'contradiction': 0.0}\n",
      "Human Reason: The scrum machine is a safe tool developed to make rugby practice safer.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 14 ---\n",
      "Section: test_r1\n",
      "Premise: Melissa Duck is an animated cartoon character in the Warner Brothers \"Looney Tunes\" and \"Merrie Melodies\" series of cartoons and the animated television series \"Baby Looney Tunes\". She is featured as main character Daffy Duck's blonde girlfriend in several cartoon shorts but is only referred to as Melissa in one, \"The Scarlet Pumpernickel\", where she is voiced by Marian Richman.\n",
      "Hypothesis: Richman voiced Daffy Duck in several cartoons\n",
      "Model Predicted: contradiction\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 1.3, 'neutral': 41.1, 'contradiction': 57.6}\n",
      "Human Reason: We don't know who voiced Daffy Duck\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 15 ---\n",
      "Section: test_r1\n",
      "Premise: Emma Catherine Rigby (born 26 September 1989) is an English actress. She is best known for playing the role of Hannah Ashworth in long-running soap opera \"Hollyoaks\", Gemma Roscoe in BBC One drama series \"Prisoners' Wives\" and as the Red Queen in American fantasy-drama \"Once Upon a Time in Wonderland.\"\n",
      "Hypothesis: Emma Catherine Rigby was born in England in 1989.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 71.0, 'neutral': 28.9, 'contradiction': 0.1}\n",
      "Human Reason: This statement is neither definitely correct nor definitely incorrect because although the information provided indicates Emma Catherine Rigby is English, it does not specify that she was born in England.  She could have, for example, been born to English parents while they lived in another country.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 16 ---\n",
      "Section: test_r2\n",
      "Premise: The Wolfsonian–Florida International University or The Wolfsonian-FIU, located in the heart of the Art Deco District of Miami Beach, Florida, is a museum, library and research center that uses its collection to illustrate the persuasive power of art and design. For fifteen years, The Wolfsonian has been a division within Florida International University.\n",
      "Hypothesis: For the total amount of years that is equivalent to five multiplied by five, The Wolfsonian has been a division within Florida International University.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 98.2, 'neutral': 0.1, 'contradiction': 1.6}\n",
      "Human Reason: five times five equals 25 but the correct answer is fifteen so it was incorrect but the AI recognized the wording but not able to calculate to see if the math is correct.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 17 ---\n",
      "Section: test_r2\n",
      "Premise: O lieb, so lang du lieben kannst is a poem written by Ferdinand Freiligrath, a 19th-century German writer. In 1847, Hungarian composer Franz Liszt set the poem to music (soprano voice and piano), and eventually adapted it into his famous Liebesträume No. 3. The work is one of Liszt's most famous and poignant. \"Liebesträume\" in German means \"Dreams of Love\".\n",
      "Hypothesis: Dreams of Love showcased more than one musical number\n",
      "Model Predicted: entailment\n",
      "Correct Answer: neutral\n",
      "Prediction Confidence: {'entailment': 98.2, 'neutral': 1.6, 'contradiction': 0.2}\n",
      "Human Reason: While it states that Dreams of Love had Liebestraume No 3, it did not state it had other numbers in it. Composers often stick a piece labeled 'number x' without adding the preceeding numbers. It is simply a version. The model probably got it wrong because it assumed '3' was part of a continuing series\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 18 ---\n",
      "Section: test_r3\n",
      "Premise: I have no problem with the people on welfare having an opportunity for a job, but I do not agree with the fact that the government, because of problems due to its changes to employment insurance, takes people and, to get them off welfare because changes to employment insurance have resulted in an increase in the number of people on welfare, and sends them working in order to get them on employment insurance and off the provincial rolls.\n",
      "Hypothesis: Changes to employment insurance by the government had unforeseen consequences.\n",
      "Model Predicted: neutral\n",
      "Correct Answer: entailment\n",
      "Prediction Confidence: {'entailment': 4.6, 'neutral': 95.2, 'contradiction': 0.2}\n",
      "Human Reason: The text states the government made changes to employment insurance that increased the number of people on welfare.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 19 ---\n",
      "Section: test_r3\n",
      "Premise: The Anubis Shrine was part of the grave gods of Tutankhamun (18th Dynasty, New Kingdom). The tomb (KV62) was discovered almost intact on 4 November 1922 in the Valley of the Kings in west Thebes by Howard Carter. Today the object, with the find number 261, is an exhibit at the Egyptian Museum in Cairo, with the inventory number JE 61444.\n",
      "Hypothesis: The shrine was part of the gods of hamun\n",
      "Model Predicted: entailment\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 99.4, 'neutral': 0.3, 'contradiction': 0.2}\n",
      "Human Reason: The gods of hamun do not exist making the statement incorrect. The model has problems with the spellings of words\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- ERROR 20 ---\n",
      "Section: test_r1\n",
      "Premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983).\n",
      "Hypothesis: Askod Makarov was a Russian ball dancer and teacher in the 1960's and 70's.\n",
      "Model Predicted: entailment\n",
      "Correct Answer: contradiction\n",
      "Prediction Confidence: {'entailment': 83.2, 'neutral': 9.4, 'contradiction': 7.4}\n",
      "Human Reason: Askod Makarov was a ballet dancer not a ball dancer.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ 20 error samples displayed!\n",
      "📝 Copy these examples and send them for detailed analysis.\n"
     ]
    }
   ],
   "source": [
    "# Task 1.2: Sample 20 Errors for Investigation\n",
    "\n",
    "import random\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TASK 1.2: SAMPLING ERRORS FOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collect all errors from all test sections\n",
    "all_errors = []\n",
    "\n",
    "for section_name, section_data in all_results.items():\n",
    "    predictions = section_data['predictions']\n",
    "    for pred in predictions:\n",
    "        if pred['pred_label'] != pred['gold_label']:  # This is an error\n",
    "            error_info = {\n",
    "                'section': section_name,\n",
    "                'premise': pred['premise'],\n",
    "                'hypothesis': pred['hypothesis'],\n",
    "                'predicted': pred['pred_label'],\n",
    "                'gold': pred['gold_label'],\n",
    "                'reason': pred['reason'],\n",
    "                'prediction_scores': pred['prediction']\n",
    "            }\n",
    "            all_errors.append(error_info)\n",
    "\n",
    "print(f\"📊 Total errors found: {len(all_errors)}\")\n",
    "for section in ['test_r1', 'test_r2', 'test_r3']:\n",
    "    section_errors = [e for e in all_errors if e['section'] == section]\n",
    "    print(f\"   {section}: {len(section_errors)} errors\")\n",
    "\n",
    "# Sample 20 errors for analysis\n",
    "random.seed(42)  # For reproducible results\n",
    "sampled_errors = random.sample(all_errors, min(20, len(all_errors)))\n",
    "\n",
    "print(f\"\\n🔍 Here are {len(sampled_errors)} randomly sampled errors for investigation:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display each error clearly\n",
    "for i, error in enumerate(sampled_errors, 1):\n",
    "    print(f\"\\n--- ERROR {i} ---\")\n",
    "    print(f\"Section: {error['section']}\")\n",
    "    print(f\"Premise: {error['premise']}\")\n",
    "    print(f\"Hypothesis: {error['hypothesis']}\")\n",
    "    print(f\"Model Predicted: {error['predicted']}\")\n",
    "    print(f\"Correct Answer: {error['gold']}\")\n",
    "    print(f\"Prediction Confidence: {error['prediction_scores']}\")\n",
    "    print(f\"Human Reason: {error['reason']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n✅ {len(sampled_errors)} error samples displayed!\")\n",
    "print(\"📝 Copy these examples and send them for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8b058",
   "metadata": {},
   "source": [
    "### Task 1.2: Reasons the model made a mistake  \n",
    "\n",
    "\n",
    "#### 1. **Mathematical/Numerical Reasoning Failures** (25%)\n",
    "**Errors: 2, 3, 8, 14, 18**\n",
    "\n",
    "- **Error 2**: Failed age calculation (1990→2014 = 24 years, not 18)\n",
    "- **Error 3**: Failed age calculation (1973→2019 = 45-46 years) \n",
    "- **Error 8**: Failed duration calculation (March 1990→Dec 1992 ≈ 2.75 years, not 3)\n",
    "- **Error 14**: Failed reverse calculation (8th year in 1938 → started 1930)\n",
    "- **Error 18**: Failed counting (8 named actors = 8 protagonists)\n",
    "\n",
    "**Pattern**: Model struggles with basic arithmetic, date calculations, and counting tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Missing Information → False Contradiction** (40%)\n",
    "**Errors: 1, 6, 10, 11, 12, 13, 15, 17**\n",
    "\n",
    "- **Error 1**: \"Only 3 countries\" → Model sees contradiction instead of neutral (no exclusivity stated)\n",
    "- **Error 6**: Extension name → Model assumes contradiction from missing info\n",
    "- **Error 10**: \"Only in branch\" → Model contradicts unstated restriction  \n",
    "- **Error 11**: Start date claim → Model treats missing date as contradiction\n",
    "- **Error 12**: Future preferences → Model assumes contradiction from unknown future\n",
    "- **Error 13**: Profession claim → Model contradicts unstated profession\n",
    "- **Error 15**: Meeting location → Model contradicts unstated location\n",
    "- **Error 17**: Career continuation → Model contradicts missing timeline info\n",
    "\n",
    "**Pattern**: Model is overly aggressive in predicting contradictions when information is simply absent. Should predict \"neutral\" but defaults to \"contradiction.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Temporal Reasoning Errors** (15%)\n",
    "**Errors: 5, 16, 19**\n",
    "\n",
    "- **Error 5**: Marriage timeline vs TV show timing → Can't handle missing temporal overlap\n",
    "- **Error 16**: Past tense description → Incorrectly infers current non-existence  \n",
    "- **Error 19**: \"Found as kitten\" → Incorrectly assumes current state\n",
    "\n",
    "**Pattern**: Model struggles with temporal states, timeline inference, and distinguishing past vs present states.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Basic Reading Comprehension** (10%)\n",
    "**Errors: 4, 20**\n",
    "\n",
    "- **Error 4**: Clear text states \"Van Morrison song on album\" → Model predicts contradiction\n",
    "- **Error 20**: Text confusion between \"large part\" vs \"tinny population\"\n",
    "\n",
    "**Pattern**: Fundamental misreading of clear, direct statements.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Complex Linguistic Reasoning** (5%)\n",
    "**Error: 9**\n",
    "\n",
    "- **Error 9**: Wordplay with \"sovereignty\" as publication name → Model missed that core claim (Britain refused to address sovereignty) remains true despite confusing wording\n",
    "\n",
    "**Pattern**: Difficulty with complex sentence structures and embedded meaning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Opinion vs Fact Confusion** (5%) \n",
    "**Error: 7**\n",
    "\n",
    "- **Error 7**: \"Should be called\" (opinion) → Model treats normative statement as factual contradiction\n",
    "\n",
    "**Pattern**: Cannot distinguish between factual claims and opinion statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b56fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [03:04<00:00,  6.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "key = \"test_r3\"\n",
    "# Filter out missing reasons\n",
    "filtered_data = dataset[key].filter(lambda x: x[\"reason\"] is not None and x[\"reason\"] != \"\")\n",
    "\n",
    "\n",
    "results = evaluate_on_dataset(filtered_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save baseline model predictions\n",
    "with open(\"baseline_preds.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - hw2)",
   "language": "python",
   "name": "hw2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
