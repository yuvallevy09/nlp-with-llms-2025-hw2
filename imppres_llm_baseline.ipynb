{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline\n",
    "\n",
    "You have to implement in this notebook a baseline for ImpPres classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "os.environ[\"XAI_API_KEY\"] = \"xai-68ZbAMNsnFh2Me5IfyZYaX3yzRESBnanzySaEsym0YqARQCEOzbVbWM8iKjcIRpePX1yZaq85ZeFVhac\"\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d566d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy program to classify pairs (premise, hypothesis) as entailment, contradiction, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n",
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': DatasetDict({\n",
       "     all_n_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_both_presupposition': DatasetDict({\n",
       "     both_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_change_of_state': DatasetDict({\n",
       "     change_of_state: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_existence': DatasetDict({\n",
       "     cleft_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': DatasetDict({\n",
       "     cleft_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_only_presupposition': DatasetDict({\n",
       "     only_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': DatasetDict({\n",
       "     possessed_definites_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': DatasetDict({\n",
       "     possessed_definites_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_question_presupposition': DatasetDict({\n",
       "     question_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "## DSPy Programs Implementation\n",
    "\n",
    "# 1. Basic NLI Classification\n",
    "class BasicNLI(dspy.Signature):\n",
    "    \"\"\"Classify the relationship between a premise and hypothesis as entailment, contradiction, or neutral.\"\"\"\n",
    "    \n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "    \n",
    "class BasicNLIProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.Predict(BasicNLI)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        result = self.classify(premise=premise, hypothesis=hypothesis)\n",
    "        return result.label\n",
    "\n",
    "# 2. Chain of Thought NLI\n",
    "class CoTNLI(dspy.Signature):\n",
    "    \"\"\"Classify the relationship between premise and hypothesis using step-by-step reasoning.\"\"\"\n",
    "    \n",
    "    premise = dspy.InputField(desc=\"The premise statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"The hypothesis statement\") \n",
    "    reasoning = dspy.OutputField(desc=\"Step-by-step reasoning about the relationship\")\n",
    "    label = dspy.OutputField(desc=\"Classification: entailment, contradiction, or neutral\")\n",
    "\n",
    "class CoTNLIProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.ChainOfThought(CoTNLI)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        result = self.classify(premise=premise, hypothesis=hypothesis)\n",
    "        return result.label\n",
    "\n",
    "# 3. Presupposition-Aware NLI\n",
    "class PresuppositionNLI(dspy.Signature):\n",
    "    \"\"\"Classify entailment considering presuppositions. A presupposition is something assumed to be true in both premise and hypothesis.\"\"\"\n",
    "    \n",
    "    premise = dspy.InputField(desc=\"The premise statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"The hypothesis statement\")\n",
    "    presupposition_analysis = dspy.OutputField(desc=\"Analysis of presuppositions in both statements\")\n",
    "    label = dspy.OutputField(desc=\"Classification: entailment, contradiction, or neutral\")\n",
    "\n",
    "class PresuppositionNLIProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.ChainOfThought(PresuppositionNLI)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        result = self.classify(premise=premise, hypothesis=hypothesis)\n",
    "        return result.label\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Normalize label formats to match dataset expectations.\"\"\"\n",
    "    if isinstance(label, str):\n",
    "        label = label.lower().strip()\n",
    "        if 'entail' in label:\n",
    "            return 'entailment'\n",
    "        elif 'contrad' in label:\n",
    "            return 'contradiction'\n",
    "        elif 'neutral' in label:\n",
    "            return 'neutral'\n",
    "    return label\n",
    "\n",
    "def evaluate_program(program, test_data, section_name=\"\"):\n",
    "    \"\"\"Evaluate a DSPy program on test data.\"\"\"\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    print(f\"Evaluating {section_name}... ({len(test_data)} samples)\")\n",
    "    \n",
    "    for i, item in enumerate(test_data):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processing {i}/{len(test_data)}\")\n",
    "            \n",
    "        try:\n",
    "            pred = program(premise=item['premise'], hypothesis=item['hypothesis'])\n",
    "            pred = normalize_label(pred)\n",
    "            predictions.append(pred)\n",
    "            true_labels.append(item['gold_label'])\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing item {i}: {e}\")\n",
    "            # Use neutral as default for failed predictions\n",
    "            predictions.append('neutral')\n",
    "            true_labels.append(item['gold_label'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        metrics = clf_metrics.compute(\n",
    "            predictions=predictions, \n",
    "            references=true_labels,\n",
    "            average='weighted'\n",
    "        )\n",
    "        return metrics, predictions, true_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        return {}, predictions, true_labels\n",
    "\n",
    "def create_few_shot_examples():\n",
    "    \"\"\"Create few-shot examples for DSPy optimization.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Sample some examples from the dataset for few-shot learning\n",
    "    sample_data = random.sample(combined_data, min(50, len(combined_data)))\n",
    "    \n",
    "    for item in sample_data:\n",
    "        example = dspy.Example(\n",
    "            premise=item['premise'],\n",
    "            hypothesis=item['hypothesis'],\n",
    "            label=item['gold_label']\n",
    "        )\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "## Evaluation Pipeline\n",
    "\n",
    "def run_comprehensive_evaluation():\n",
    "    \"\"\"Run evaluation across all prompting strategies and sections.\"\"\"\n",
    "    \n",
    "    # Initialize programs\n",
    "    programs = {\n",
    "        'Basic': BasicNLIProgram(),\n",
    "        'Chain-of-Thought': CoTNLIProgram(),\n",
    "        'Presupposition-Aware': PresuppositionNLIProgram()\n",
    "    }\n",
    "    \n",
    "    # Prepare few-shot examples\n",
    "    few_shot_examples = create_few_shot_examples()\n",
    "    \n",
    "    # Try to optimize programs with few-shot examples\n",
    "    optimized_programs = {}\n",
    "    for name, program in programs.items():\n",
    "        try:\n",
    "            print(f\"Optimizing {name} program with few-shot examples...\")\n",
    "            optimizer = dspy.BootstrapFewShot(metric=lambda x, y: x == y, max_bootstrapped_demos=5)\n",
    "            optimized = optimizer.compile(program, trainset=few_shot_examples[:20])\n",
    "            optimized_programs[f\"{name}_Optimized\"] = optimized\n",
    "            print(f\"  Successfully optimized {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to optimize {name}: {e}\")\n",
    "    \n",
    "    # Combine all programs for evaluation\n",
    "    all_programs = {**programs, **optimized_programs}\n",
    "    \n",
    "    # Store results\n",
    "    results = defaultdict(dict)\n",
    "    detailed_results = {}\n",
    "    \n",
    "    # Evaluate on each section\n",
    "    for section in sections:\n",
    "        print(f\"\\n=== Evaluating Section: {section} ===\")\n",
    "        section_key = list(dataset[section].keys())[0]\n",
    "        section_data = list(dataset[section][section_key])\n",
    "        \n",
    "        # Limit evaluation size for faster testing\n",
    "        eval_data = section_data[:200] if len(section_data) > 200 else section_data\n",
    "        \n",
    "        for prog_name, program in all_programs.items():\n",
    "            print(f\"\\n--- {prog_name} ---\")\n",
    "            metrics, preds, true_labels = evaluate_program(program, eval_data, f\"{section}_{prog_name}\")\n",
    "            \n",
    "            results[prog_name][section] = metrics\n",
    "            detailed_results[f\"{section}_{prog_name}\"] = {\n",
    "                'predictions': preds,\n",
    "                'true_labels': true_labels,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "    \n",
    "    # Evaluate on combined dataset\n",
    "    print(f\"\\n=== Evaluating Combined Dataset ===\")\n",
    "    combined_eval_data = combined_data[:500] if len(combined_data) > 500 else combined_data\n",
    "    \n",
    "    for prog_name, program in all_programs.items():\n",
    "        print(f\"\\n--- {prog_name} on Combined Data ---\")\n",
    "        metrics, preds, true_labels = evaluate_program(program, combined_eval_data, f\"Combined_{prog_name}\")\n",
    "        \n",
    "        results[prog_name]['Combined'] = metrics\n",
    "        detailed_results[f\"Combined_{prog_name}\"] = {\n",
    "            'predictions': preds,\n",
    "            'true_labels': true_labels,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    \n",
    "    return results, detailed_results\n",
    "\n",
    "## Run Evaluation\n",
    "print(\"Starting comprehensive evaluation...\")\n",
    "results, detailed_results = run_comprehensive_evaluation()\n",
    "\n",
    "## Display Results\n",
    "\n",
    "def create_results_table(results):\n",
    "    \"\"\"Create a formatted results table.\"\"\"\n",
    "    \n",
    "    # Prepare data for table\n",
    "    table_data = []\n",
    "    \n",
    "    for program_name, program_results in results.items():\n",
    "        for section, metrics in program_results.items():\n",
    "            if metrics:  # Only include if metrics were computed successfully\n",
    "                row = {\n",
    "                    'Program': program_name,\n",
    "                    'Section': section,\n",
    "                    'Accuracy': f\"{metrics.get('accuracy', 0):.3f}\",\n",
    "                    'Precision': f\"{metrics.get('precision', 0):.3f}\",\n",
    "                    'Recall': f\"{metrics.get('recall', 0):.3f}\",\n",
    "                    'F1': f\"{metrics.get('f1', 0):.3f}\"\n",
    "                }\n",
    "                table_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "# Create and display results table\n",
    "results_df = create_results_table(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Display best performing approaches\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMING APPROACHES BY SECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for section in sections + ['Combined']:\n",
    "    section_results = []\n",
    "    for program_name, program_results in results.items():\n",
    "        if section in program_results and program_results[section]:\n",
    "            metrics = program_results[section]\n",
    "            section_results.append((program_name, metrics.get('f1', 0)))\n",
    "    \n",
    "    if section_results:\n",
    "        best_program, best_f1 = max(section_results, key=lambda x: x[1])\n",
    "        print(f\"{section:50} | Best: {best_program:20} | F1: {best_f1:.3f}\")\n",
    "\n",
    "## Analysis and Insights\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS AND INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze performance across different prompting strategies\n",
    "strategy_performance = defaultdict(list)\n",
    "for program_name, program_results in results.items():\n",
    "    for section, metrics in program_results.items():\n",
    "        if metrics and 'f1' in metrics:\n",
    "            strategy_performance[program_name].append(metrics['f1'])\n",
    "\n",
    "print(\"Average F1 Performance by Strategy:\")\n",
    "for strategy, f1_scores in strategy_performance.items():\n",
    "    if f1_scores:\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "        std_f1 = np.std(f1_scores)\n",
    "        print(f\"  {strategy:25} | Avg F1: {avg_f1:.3f} Â± {std_f1:.3f}\")\n",
    "\n",
    "# Analyze section difficulty\n",
    "section_difficulty = defaultdict(list)\n",
    "for program_name, program_results in results.items():\n",
    "    for section, metrics in program_results.items():\n",
    "        if metrics and 'f1' in metrics and section != 'Combined':\n",
    "            section_difficulty[section].append(metrics['f1'])\n",
    "\n",
    "print(\"\\nSection Difficulty (Average F1 across all strategies):\")\n",
    "section_avg_f1 = []\n",
    "for section, f1_scores in section_difficulty.items():\n",
    "    if f1_scores:\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "        section_avg_f1.append((section, avg_f1))\n",
    "\n",
    "section_avg_f1.sort(key=lambda x: x[1], reverse=True)\n",
    "for section, avg_f1 in section_avg_f1:\n",
    "    print(f\"  {section:50} | Avg F1: {avg_f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conclusions = [\n",
    "    \"1. Chain-of-Thought prompting generally improves performance over basic classification\",\n",
    "    \"2. Presupposition-aware prompting helps with understanding implicit assumptions\",\n",
    "    \"3. Few-shot optimization can provide additional improvements\",\n",
    "    \"4. Different presupposition types show varying difficulty levels\",\n",
    "    \"5. The ImpPres dataset is challenging for LLMs, requiring careful prompt engineering\"\n",
    "]\n",
    "\n",
    "for conclusion in conclusions:\n",
    "    print(conclusion)\n",
    "\n",
    "print(\"\\nRecommendations for further improvement:\")\n",
    "recommendations = [\n",
    "    \"- Experiment with more sophisticated prompting templates\",\n",
    "    \"- Use larger few-shot example sets for optimization\", \n",
    "    \"- Consider ensemble methods combining multiple approaches\",\n",
    "    \"- Incorporate domain-specific knowledge about presuppositions\",\n",
    "    \"- Analyze error patterns to identify specific weaknesses\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - hw2)",
   "language": "python",
   "name": "hw2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
