{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "\n",
    "# Set the API key directly in notebook\n",
    "os.environ[\"XAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Configure DSPy\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4e77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class EntailmentSignature(dspy.Signature):\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()\n",
    "\n",
    "class EntailmentClassifier(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.Predict(EntailmentSignature)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        return self.classify(premise=premise, hypothesis=hypothesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb94a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "# Shuffle and sample 30 examples from dev_r3\n",
    "dev_r3_sample = dataset[\"dev_r3\"].shuffle(seed=42).select(range(30))\n",
    "\n",
    "\n",
    "train_examples = [\n",
    "    dspy.Example(\n",
    "        premise=ex[\"premise\"],\n",
    "        hypothesis=ex[\"hypothesis\"],\n",
    "        label=label_names[ex[\"label\"]]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")  \n",
    "    for ex in dev_r3_sample\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59031a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:34 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 10\n",
      "minibatch: False\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 24\n",
      "\n",
      "2025/07/17 17:40:34 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/07/17 17:40:34 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/07/17 17:40:34 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 226.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 452.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 525.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:00<00:00, 594.29it/s]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `premise`, `hypothesis`, produce the fields `label`.\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are an expert in natural language inference (NLI). Given a premise and a hypothesis, your task is to classify the relationship between them by producing a single label from the following options: 'entailment' (the hypothesis logically follows from the premise), 'neutral' (the hypothesis is neither supported nor contradicted by the premise), or 'contradiction' (the hypothesis conflicts with the premise). Analyze the premise and hypothesis carefully, considering logical implications, factual consistency, and any potential contradictions.\n",
      "\n",
      "To reason step by step:\n",
      "1. Read the premise and hypothesis thoroughly.\n",
      "2. Determine if the hypothesis is directly implied by the premise (entailment), unrelated or uncertain (neutral), or in direct opposition (contradiction).\n",
      "3. Output only the label in the specified format.\n",
      "\n",
      "Premise: [insert premise here]\n",
      "Hypothesis: [insert hypothesis here]\n",
      "\n",
      "Output: Label: [your chosen label, e.g., 'entailment']\n",
      "\n",
      "Ensure your response is concise and directly provides the label based on the definitions above.\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are an expert in natural language inference (NLI). Your task is to analyze the relationship between a given premise and a hypothesis, and classify it as one of the following: 'entailment' (the hypothesis logically follows from the premise), 'neutral' (the hypothesis neither follows from nor contradicts the premise), or 'contradiction' (the hypothesis contradicts the premise). \n",
      "\n",
      "To perform this task:\n",
      "- Carefully read the premise and hypothesis provided.\n",
      "- Reason step by step about the logical relationship, considering the meanings, implications, and any potential ambiguities in the texts.\n",
      "- Output only the label in the format: \"Label: [entailment/neutral/contradiction]\".\n",
      "\n",
      "For example:\n",
      "- If the premise is \"That time John Grisham defended men who watch child porn...\" and the hypothesis is \"John Grisham watches kiddie porn.\", the label should be \"neutral\" because the premise discusses his defense but does not confirm his personal involvement.\n",
      "- If the premise is \"Well, I think again that illustrates why the authority is vested in Congress...\" and the hypothesis is \"Decisions Congress make impact society on a broad scale\", the label should be \"entailment\" as the premise implies broad societal effects.\n",
      "- If the premise is \"Tonight the topic will be domestic affairs...\" and the hypothesis is \"the topic will be domestic affairs, moderated on rules and questions agreed to by the candidates.\", the label should be \"contradiction\" because the premise states the questions were not agreed to by the candidates.\n",
      "\n",
      "Given the fields `premise` and `hypothesis`, produce the field `label`.\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 10 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 565.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 70.83\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuval/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 509.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 18.00 / 24 (75.0%): 100%|██████████| 24/24 [00:00<00:00, 539.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 18 / 24 (75.0%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 75.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 75.0 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.00 / 24 (66.7%): 100%|██████████| 24/24 [00:00<00:00, 488.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 16 / 24 (66.7%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 66.67 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 16.00 / 24 (66.7%): 100%|██████████| 24/24 [00:00<00:00, 281.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 16 / 24 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 66.67 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 6 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 455.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67, 70.83]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.00 / 24 (75.0%): 100%|██████████| 24/24 [00:00<00:00, 2516.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 18 / 24 (75.0%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 75.0 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67, 70.83, 75.0]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 8 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 505.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67, 70.83, 75.0, 70.83]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 9 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 15.00 / 24 (62.5%): 100%|██████████| 24/24 [00:00<00:00, 275.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 15 / 24 (62.5%)\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.5 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67, 70.83, 75.0, 70.83, 62.5]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 10 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 1976.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67, 70.83, 75.0, 70.83, 62.5, 70.83]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 11 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.00 / 24 (66.7%): 100%|██████████| 24/24 [00:00<00:00, 484.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.evaluate.evaluate: Average Metric: 16 / 24 (66.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 66.67 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 70.83, 75.0, 66.67, 66.67, 70.83, 75.0, 70.83, 62.5, 70.83, 66.67]\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 75.0\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/07/17 17:40:35 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 75.0!\n"
     ]
    }
   ],
   "source": [
    "from dspy import MIPROv2\n",
    "\n",
    "def exact_match(pred, gold, trace=None):\n",
    "    # Fixes argument confusion — extract the label field from Prediction and Example\n",
    "    try:\n",
    "        pred_label = getattr(pred, 'label', pred)\n",
    "        gold_label = getattr(gold, 'label', gold)\n",
    "\n",
    "        if isinstance(pred_label, dspy.Example):\n",
    "            pred_label = getattr(pred_label, 'label', pred_label)\n",
    "        if isinstance(gold_label, dspy.Example):\n",
    "            gold_label = getattr(gold_label, 'label', gold_label)\n",
    "\n",
    "        pred_str = str(pred_label).strip().lower()\n",
    "        gold_str = str(gold_label).strip().lower()\n",
    "\n",
    "        return pred_str == gold_str\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "dspy_module = EntailmentClassifier()\n",
    "\n",
    "optimizer = MIPROv2(metric=exact_match)\n",
    "\n",
    "# Compile the module with optimization on the sample set\n",
    "optimized_dspy_module = optimizer.compile(\n",
    "    dspy_module,\n",
    "    trainset=train_examples,\n",
    "    requires_permission_to_run=False  # avoids prompt for Grok cost confirmation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a74cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/1200 examples...\n",
      "Processed 50/1200 examples...\n",
      "Processed 100/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 120/1200 [00:00<00:00, 1193.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 150/1200 examples...\n",
      "Processed 200/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 265/1200 [00:00<00:00, 1341.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 250/1200 examples...\n",
      "Processed 300/1200 examples...\n",
      "Processed 350/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 400/1200 [00:00<00:00, 1152.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400/1200 examples...\n",
      "Processed 450/1200 examples...\n",
      "Processed 500/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 557/1200 [00:00<00:00, 1300.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 550/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 720/1200 [00:00<00:00, 1410.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600/1200 examples...\n",
      "Processed 650/1200 examples...\n",
      "Processed 700/1200 examples...\n",
      "Processed 750/1200 examples...\n",
      "Processed 800/1200 examples...\n",
      "Processed 850/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 878/1200 [00:00<00:00, 1463.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 900/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 1037/1200 [00:00<00:00, 1501.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 950/1200 examples...\n",
      "Processed 1000/1200 examples...\n",
      "Processed 1050/1200 examples...\n",
      "Processed 1100/1200 examples...\n",
      "Processed 1150/1200 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:00<00:00, 1415.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_r3 = dataset[\"test_r3\"].filter(lambda x: x[\"reason\"] is not None and x[\"reason\"] != \"\")\n",
    "\n",
    "dspy_results = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_r3)):\n",
    "    output = optimized_dspy_module(premise=example[\"premise\"], hypothesis=example[\"hypothesis\"])\n",
    "   \n",
    "    dspy_results.append({\n",
    "        \"premise\": example[\"premise\"],\n",
    "        \"hypothesis\": example[\"hypothesis\"],\n",
    "        \"pred_label\": output.label,\n",
    "        \"gold_label\": label_names[example[\"label\"]],\n",
    "    })\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i}/{len(test_r3)} examples...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbacfdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7033, 'precision': 0.7461, 'recall': 0.703, 'f1': 0.7086}\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "\n",
    "pred_labels = [label2id[r['pred_label']] for r in dspy_results]\n",
    "gold_labels = [label2id[r['gold_label']] for r in dspy_results]\n",
    "\n",
    "dspy_metrics = {\n",
    "    \"accuracy\": accuracy.compute(predictions=pred_labels, references=gold_labels)[\"accuracy\"],\n",
    "    \"precision\": precision.compute(predictions=pred_labels, references=gold_labels, average=\"macro\")[\"precision\"],\n",
    "    \"recall\": recall.compute(predictions=pred_labels, references=gold_labels, average=\"macro\")[\"recall\"],\n",
    "    \"f1\": f1.compute(predictions=pred_labels, references=gold_labels, average=\"macro\")[\"f1\"],\n",
    "}\n",
    "print({k: round(v, 4) for k, v in dspy_metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea26416f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct (both correct)</th>\n",
       "      <th>Correct1 (baseline only)</th>\n",
       "      <th>Correct2 (DSPy only)</th>\n",
       "      <th>Incorrect (both wrong)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>454</td>\n",
       "      <td>140</td>\n",
       "      <td>390</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Correct (both correct)  Correct1 (baseline only)  Correct2 (DSPy only)  \\\n",
       "0                     454                       140                   390   \n",
       "\n",
       "   Incorrect (both wrong)  \n",
       "0                     216  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"baseline_preds.pkl\", \"rb\") as f:\n",
    "    baseline_results = pickle.load(f)\n",
    "\n",
    "def compute_agreement(baseline_results, dspy_results):\n",
    "    assert len(baseline_results) == len(dspy_results)\n",
    "\n",
    "    correct_both = 0\n",
    "    correct_baseline = 0\n",
    "    correct_dspy = 0\n",
    "    incorrect_both = 0\n",
    "\n",
    "    for b, d in zip(baseline_results, dspy_results):\n",
    "        gold = b[\"gold_label\"]\n",
    "        pred1 = b[\"pred_label\"]\n",
    "        pred2 = d[\"pred_label\"]\n",
    "\n",
    "        is1 = pred1 == gold\n",
    "        is2 = pred2 == gold\n",
    "\n",
    "        if is1 and is2:\n",
    "            correct_both += 1\n",
    "        elif is1 and not is2:\n",
    "            correct_baseline += 1\n",
    "        elif not is1 and is2:\n",
    "            correct_dspy += 1\n",
    "        else:\n",
    "            incorrect_both += 1\n",
    "\n",
    "    return {\n",
    "        \"Correct (both correct)\": correct_both,\n",
    "        \"Correct1 (baseline only)\": correct_baseline,\n",
    "        \"Correct2 (DSPy only)\": correct_dspy,\n",
    "        \"Incorrect (both wrong)\": incorrect_both,\n",
    "    }\n",
    "\n",
    "pd.DataFrame([compute_agreement(baseline_results, dspy_results)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - hw2)",
   "language": "python",
   "name": "hw2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
