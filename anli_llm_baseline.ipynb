{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "# Set the API key directly in notebook\n",
    "os.environ[\"XAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Configure DSPy\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "\n",
    "class Classify(dspy.Signature):\n",
    "    \"\"\"Classify the relationship between premise and hypothesis as entailment, neutral, or contradiction.\"\"\"\n",
    "    premise: str = dspy.InputField()\n",
    "    hypothesis: str = dspy.InputField()\n",
    "    label: Literal['entailment', 'neutral', 'contradiction'] = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30507ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DSPy Module\n",
    "class NLIClassifier(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.Predict(Classify)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        return self.classify(premise=premise, hypothesis=hypothesis)\n",
    "\n",
    "# Initialize the classifier\n",
    "nli_classifier = NLIClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb94a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "\n",
    "# Shuffle and sample 50 examples from dev_r3\n",
    "dev_r3_sample = dataset[\"dev_r3\"].shuffle(seed=42).select(range(30))\n",
    "\n",
    "\n",
    "train_examples = [\n",
    "    dspy.Example(\n",
    "        premise=ex[\"premise\"],\n",
    "        hypothesis=ex[\"hypothesis\"],\n",
    "        label=label_names[ex[\"label\"]]\n",
    "    ).with_inputs(\"premise\", \"hypothesis\")  \n",
    "    for ex in dev_r3_sample\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59031a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:49:42 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 10\n",
      "minibatch: False\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 24\n",
      "\n",
      "2025/06/30 16:49:42 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/06/30 16:49:42 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/06/30 16:49:42 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:26<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:11<00:11,  3.77s/it]\n",
      "2025/06/30 16:51:08 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/06/30 16:51:08 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `premise`, `hypothesis`, produce the fields `label`.\n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are an expert in Natural Language Inference (NLI). Your task is to analyze the given premise and hypothesis and determine their logical relationship. The possible labels are:\n",
      "\n",
      "- 'entailment': The hypothesis logically follows from or is supported by the premise.\n",
      "- 'neutral': The hypothesis is neither supported nor contradicted by the premise; it may be unrelated or indeterminate.\n",
      "- 'contradiction': The hypothesis conflicts with or is negated by the premise.\n",
      "\n",
      "Consider the following examples for guidance:\n",
      "\n",
      "1. Premise: \"That time John Grisham defended men who watch child porn John Grisham's disturbing statements defending child porn consumers are coming back to haunt him. In a bizarre 2014 interview, the best-selling author said most men who watch kiddie porn are... John Grisham's bizarre defense of kiddie porn viewers 'We've got prisons now filled with guys my age, 60-year-old white men, in prison, who have never harmed anyone,' the best-selling author said.\"  \n",
      "   Hypothesis: \"John Grisham watches kiddie porn.\"  \n",
      "   Label: neutral  (The premise discusses Grisham's defense but does not confirm or deny that he watches it.)\n",
      "\n",
      "2. Premise: \"Well, I think again that illustrates why the authority is vested in Congress to make these judgments rather than in courts to make these affect whether the judgments, because we're not talking about the effect on an individual author, or an individual creator. What the Framers of the Constitution were concerned about is a gross judgment with respect to what might generally provide incentives to the population --\"  \n",
      "   Hypothesis: \"Decisions Congress make impact society on a broad scale.\"  \n",
      "   Label: entailment  (The premise implies that Congress's judgments have wide-reaching effects.)\n",
      "\n",
      "3. Premise: \"Tonight the topic will be domestic affairs, but the format will be the same as that first debate. I'll moderate our discussion under detailed rules agreed to by the candidates, but the questions and the areas to be covered were chosen by me. I have not told the candidates or anyone else what they are.\"  \n",
      "   Hypothesis: \"the topic will be domestic affairs, moderated on rules and questions agreed to by the candidates.\"  \n",
      "   Label: contradiction  (The premise states that the questions were chosen by the moderator, not agreed to by the candidates.)\n",
      "\n",
      "Given the fields `premise` and `hypothesis`, produce the field `label` by reasoning step by step about the logical relationship based on the definitions above. Ensure your output is exactly one of the labels: 'entailment', 'neutral', or 'contradiction'.\n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are an expert in Natural Language Inference (NLI). Carefully analyze the relationship between the given premise and hypothesis. Determine if the hypothesis is:\n",
      "\n",
      "- 'entailment': The hypothesis logically follows from or is implied by the premise.\n",
      "- 'neutral': The hypothesis is neither supported nor contradicted by the premise; it may be possible but not directly related.\n",
      "- 'contradiction': The hypothesis conflicts with or is inconsistent with the premise.\n",
      "\n",
      "To perform this task, reason step by step based on the content and logic of the statements. Then, output only the label: 'entailment', 'neutral', or 'contradiction'.\n",
      "\n",
      "For example:\n",
      "- Premise: \"That time John Grisham defended men who watch child porn...\" Hypothesis: \"John Grisham watches kiddie porn.\" Label: neutral (The hypothesis is not directly supported or contradicted by the premise.)\n",
      "- Premise: \"Well, I think again that illustrates why the authority is vested in Congress...\" Hypothesis: \"Decisions Congress make impact society on a broad scale.\" Label: entailment (The premise implies a broad societal impact.)\n",
      "- Premise: \"Tonight the topic will be domestic affairs...\" Hypothesis: \"the topic will be domestic affairs, moderated on rules and questions agreed to by the candidates.\" Label: contradiction (The premise states the questions were chosen by the moderator, not agreed to by the candidates.)\n",
      "\n",
      "Given the fields `premise` and `hypothesis`, produce the field `label`.\n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/06/30 16:52:15 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 10 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:17<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:52:33 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:52:33 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 70.83\n",
      "\n",
      "/Users/yuval/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/06/30 16:52:33 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 15.00 / 24 (62.5%): 100%|██████████| 24/24 [00:14<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:52:47 INFO dspy.evaluate.evaluate: Average Metric: 15 / 24 (62.5%)\n",
      "2025/06/30 16:52:48 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.5 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/06/30 16:52:48 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5]\n",
      "2025/06/30 16:52:48 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:52:48 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:52:48 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:16<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:53:04 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:53:04 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/06/30 16:53:04 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83]\n",
      "2025/06/30 16:53:04 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:53:04 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:53:04 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:17<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:53:21 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:53:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/06/30 16:53:21 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83]\n",
      "2025/06/30 16:53:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:53:21 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:53:21 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 16.00 / 24 (66.7%): 100%|██████████| 24/24 [00:15<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:53:37 INFO dspy.evaluate.evaluate: Average Metric: 16 / 24 (66.7%)\n",
      "2025/06/30 16:53:37 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 66.67 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/06/30 16:53:37 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67]\n",
      "2025/06/30 16:53:37 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:53:37 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:53:37 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 6 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:19<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:53:56 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67, 70.83]\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 2114.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:53:56 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67, 70.83, 70.83]\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:53:56 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 8 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:20<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:54:17 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:54:17 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/06/30 16:54:17 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67, 70.83, 70.83, 70.83]\n",
      "2025/06/30 16:54:17 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:54:17 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:54:17 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 9 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 16.00 / 24 (66.7%): 100%|██████████| 24/24 [00:15<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:54:32 INFO dspy.evaluate.evaluate: Average Metric: 16 / 24 (66.7%)\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 66.67 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67, 70.83, 70.83, 70.83, 66.67]\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 10 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 1932.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:54:32 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67, 70.83, 70.83, 70.83, 66.67, 70.83]\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/30 16:54:32 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 11 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 17.00 / 24 (70.8%): 100%|██████████| 24/24 [00:00<00:00, 2183.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 16:54:33 INFO dspy.evaluate.evaluate: Average Metric: 17 / 24 (70.8%)\n",
      "2025/06/30 16:54:33 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 70.83 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/06/30 16:54:33 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [70.83, 62.5, 70.83, 70.83, 66.67, 70.83, 70.83, 70.83, 66.67, 70.83, 70.83]\n",
      "2025/06/30 16:54:33 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 70.83\n",
      "2025/06/30 16:54:33 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/30 16:54:33 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 70.83!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy import MIPROv2\n",
    "\n",
    "\n",
    "def exact_match(pred, gold, trace=None):\n",
    "    # Fixes argument confusion — extract the label field from Prediction and Example\n",
    "    try:\n",
    "        pred_label = getattr(pred, 'label', pred)\n",
    "        gold_label = getattr(gold, 'label', gold)\n",
    "\n",
    "\n",
    "        if isinstance(pred_label, dspy.Example):\n",
    "            pred_label = getattr(pred_label, 'label', pred_label)\n",
    "        if isinstance(gold_label, dspy.Example):\n",
    "            gold_label = getattr(gold_label, 'label', gold_label)\n",
    "\n",
    "\n",
    "        pred_str = str(pred_label).strip().lower()\n",
    "        gold_str = str(gold_label).strip().lower()\n",
    "\n",
    "\n",
    "        #print(f\"DEBUG: Comparing prediction '{pred_str}' with reference '{gold_str}'\")\n",
    "        return pred_str == gold_str\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Initialize your classifier module\n",
    "dspy_module = NLIClassifier()\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = MIPROv2(metric=exact_match)\n",
    "\n",
    "# Compile the module with optimization on the sample set\n",
    "optimized_dspy_module = optimizer.compile(\n",
    "    dspy_module,\n",
    "    trainset=train_examples,\n",
    "    requires_permission_to_run=False  # avoids prompt for Grok cost confirmation\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - hw2)",
   "language": "python",
   "name": "hw2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
