{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fc9930",
   "metadata": {},
   "source": [
    "# DSPy Tutorials\n",
    "\n",
    "From dspy.ai - these tutorials demonstrate how to build DSPy programs to perform various tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59c5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "# lm = dspy.LM('xai/grok-3-mini')\n",
    "lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7ccb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 15:12:05 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.APIConnectionError: Ollama_chatException - [Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:160\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     response = \u001b[43msync_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:747\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:726\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    723\u001b[39m     req = \u001b[38;5;28mself\u001b[39m.client.build_request(\n\u001b[32m    724\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, url, data=data, json=json, params=params, headers=headers, files=files, content=content  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    725\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOllamaError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3025\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3018\u001b[39m     api_key = (\n\u001b[32m   3019\u001b[39m         api_key\n\u001b[32m   3020\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m litellm.ollama_key\n\u001b[32m   3021\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOLLAMA_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3022\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m litellm.api_key\n\u001b[32m   3023\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3032\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama_chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001b[39;49;00m\n\u001b[32m   3040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mtriton\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:455\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001b[39m\n\u001b[32m    453\u001b[39m     sync_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    467\u001b[39m     model=model,\n\u001b[32m    468\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    477\u001b[39m     json_mode=json_mode,\n\u001b[32m    478\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:187\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:2371\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   2369\u001b[39m     error_headers = {}\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   2372\u001b[39m     error_message=error_text,\n\u001b[32m   2373\u001b[39m     status_code=status_code,\n\u001b[32m   2374\u001b[39m     headers=error_headers,\n\u001b[32m   2375\u001b[39m )\n",
      "\u001b[31mOllamaError\u001b[39m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1164\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1163\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1165\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3305\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3304\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3308\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3311\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2270\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2240\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m   2241\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(exception_provider, error_str),\n\u001b[32m   2242\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   2243\u001b[39m         model=model,\n\u001b[32m   2244\u001b[39m         request=original_exception.request,\n\u001b[32m   2245\u001b[39m     )\n\u001b[32m   2246\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAPIConnectionError\u001b[39m: litellm.APIConnectionError: Ollama_chatException - [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:69\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     68\u001b[39m     lm_kwargs[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AdapterParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# On AdapterParseError, we raise the original error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:50\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:42\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/base.py:119\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m    117\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.format(processed_signature, demos, inputs)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m outputs = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_postprocess(signature, outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/base_lm.py:96\u001b[39m, in \u001b[36mBaseLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt=\u001b[38;5;28;01mNone\u001b[39;00m, messages=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._process_lm_response(response, prompt, messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:127\u001b[39m, in \u001b[36mLM.forward\u001b[39m\u001b[34m(self, prompt, messages, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m completion, litellm_cache_args = \u001b[38;5;28mself\u001b[39m._get_cached_completion_fn(completion, cache, enable_memory_cache)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m results = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_cache_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(c.finish_reason == \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/cache.py:232\u001b[39m, in \u001b[36mrequest_cache.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# `enable_memory_cache` can be provided at call time to avoid indefinite growth.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/clients/lm.py:304\u001b[39m, in \u001b[36mlitellm_completion\u001b[39m\u001b[34m(request, num_retries, cache)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexponential_backoff_retry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream_completion()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1266\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1265\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_retries\u001b[39m\u001b[33m\"\u001b[39m] = num_retries\n\u001b[32m-> \u001b[39m\u001b[32m1266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.ContextWindowExceededError)\n\u001b[32m   1269\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1270\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m context_window_fallback_dict\n\u001b[32m   1271\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_litellm_router_call\n\u001b[32m   1272\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3343\u001b[39m, in \u001b[36mcompletion_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3340\u001b[39m     retryer = tenacity.Retrying(\n\u001b[32m   3341\u001b[39m         stop=tenacity.stop_after_attempt(num_retries), reraise=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3342\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretryer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1286\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1283\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1284\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1285\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/utils.py:1164\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1163\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1165\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/main.py:3305\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3304\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3308\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3311\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2271\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2270\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2240\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m   2241\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(exception_provider, error_str),\n\u001b[32m   2242\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   2243\u001b[39m         model=model,\n\u001b[32m   2244\u001b[39m         request=original_exception.request,\n\u001b[32m   2245\u001b[39m     )\n\u001b[32m   2246\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAPIConnectionError\u001b[39m: litellm.APIConnectionError: Ollama_chatException - [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example 1: Simple Question Answering\u001b[39;00m\n\u001b[32m      2\u001b[39m math = dspy.ChainOfThought(\u001b[33m\"\u001b[39m\u001b[33mquestion -> answer: float\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTwo dice are tossed. What is the probability that the sum equals two?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/primitives/program.py:60\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m     output.set_lm_usage(usage_tracker.get_total_tokens())\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/predict/chain_of_thought.py:38\u001b[39m, in \u001b[36mChainOfThought.forward\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:85\u001b[39m, in \u001b[36mPredict.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._get_positional_args_error_message())\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/primitives/program.py:60\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m     output.set_lm_usage(usage_tracker.get_total_tokens())\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/predict/predict.py:157\u001b[39m, in \u001b[36mPredict.forward\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m settings.context(send_stream=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         completions = \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_postprocess(completions, signature, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/chat_adapter.py:51\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJSONAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Semester 6/NLP_w_LLMs/hw2/nlp-with-llms-2025-hw2/.venv/lib/python3.11/site-packages/dspy/adapters/json_adapter.py:75\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# On any other error, we raise a RuntimeError with the original error message.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBoth structured output format and JSON mode failed. Please choose a model that supports \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`response_format` argument. Original error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Both structured output format and JSON mode failed. Please choose a model that supports `response_format` argument. Original error: litellm.APIConnectionError: Ollama_chatException - [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple Question Answering\n",
    "math = dspy.ChainOfThought(\"question -> answer: float\")\n",
    "math(question=\"Two dice are tossed. What is the probability that the sum equals two?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e238ab18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The context provides information about three different individuals named Gregory. The relevant section is [1], which discusses David Gregory, a Scottish physician and inventor. Within this section, it mentions that he inherited Kinnairdy Castle in 1664.',\n",
       "    response='Kinnairdy Castle'\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: RAG with Retrieval\n",
    "def search_wikipedia(query: str) -> list[str]:\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "rag = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "question = \"What's the name of the castle that David Gregory inherited?\"\n",
    "rag(context=search_wikipedia(question), question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6fba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='positive',\n",
       "    confidence=0.95\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3: Classification\n",
    "from typing import Literal\n",
    "\n",
    "class Classify(dspy.Signature):\n",
    "    \"\"\"Classify sentiment of a given sentence.\"\"\"\n",
    "\n",
    "    sentence: str = dspy.InputField()\n",
    "    sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField()\n",
    "    confidence: float = dspy.OutputField()\n",
    "\n",
    "classify = dspy.Predict(Classify)\n",
    "classify(sentence=\"This book was super fun to read, though not the last chapter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81e6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. Announces Latest iPhone 14\n",
      "['Latest iPhone 14', 'CEO Tim Cook']\n",
      "[{'name': 'Apple Inc.', 'type': 'Organization'}, {'name': 'iPhone 14', 'type': 'Product'}, {'name': 'Tim Cook', 'type': 'Person'}]\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Information Extraction\n",
    "class ExtractInfo(dspy.Signature):\n",
    "    \"\"\"Extract structured information from text.\"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "    title: str = dspy.OutputField()\n",
    "    headings: list[str] = dspy.OutputField()\n",
    "    entities: list[dict[str, str]] = dspy.OutputField(desc=\"a list of entities and their metadata\")\n",
    "\n",
    "module = dspy.Predict(ExtractInfo)\n",
    "\n",
    "text = \"Apple Inc. announced its latest iPhone 14 today.\" \\\n",
    "    \"The CEO, Tim Cook, highlighted its new features in a press release.\"\n",
    "response = module(text=text)\n",
    "\n",
    "print(response.title)\n",
    "print(response.headings)\n",
    "print(response.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eec4dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5762.4\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Agents\n",
    "def evaluate_math(expression: str):\n",
    "    return dspy.PythonInterpreter({}).execute(expression)\n",
    "\n",
    "def search_wikipedia(query: str):\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "react = dspy.ReAct(\"question -> answer: float\", tools=[evaluate_math, search_wikipedia])\n",
    "\n",
    "pred = react(question=\"What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?\")\n",
    "print(pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50ef1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Multi-stage pipeline\n",
    "class Outline(dspy.Signature):\n",
    "    \"\"\"Outline a thorough overview of a topic.\"\"\"\n",
    "\n",
    "    topic: str = dspy.InputField()\n",
    "    title: str = dspy.OutputField()\n",
    "    sections: list[str] = dspy.OutputField()\n",
    "    section_subheadings: dict[str, list[str]] = dspy.OutputField(desc=\"mapping from section headings to subheadings\")\n",
    "\n",
    "class DraftSection(dspy.Signature):\n",
    "    \"\"\"Draft a top-level section of an article.\"\"\"\n",
    "\n",
    "    topic: str = dspy.InputField()\n",
    "    section_heading: str = dspy.InputField()\n",
    "    section_subheadings: list[str] = dspy.InputField()\n",
    "    content: str = dspy.OutputField(desc=\"markdown-formatted section\")\n",
    "\n",
    "class DraftArticle(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.build_outline = dspy.ChainOfThought(Outline)\n",
    "        self.draft_section = dspy.ChainOfThought(DraftSection)\n",
    "\n",
    "    def forward(self, topic):\n",
    "        outline = self.build_outline(topic=topic)\n",
    "        sections = []\n",
    "        for heading, subheadings in outline.section_subheadings.items():\n",
    "            section, subheadings = f\"## {heading}\", [f\"### {subheading}\" for subheading in subheadings]\n",
    "            section = self.draft_section(topic=outline.title, section_heading=section, section_subheadings=subheadings)\n",
    "            sections.append(section.content)\n",
    "        return dspy.Prediction(title=outline.title, sections=sections)\n",
    "\n",
    "draft_article = DraftArticle()\n",
    "article = draft_article(topic=\"World Cup 2002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce448f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Introduction\n",
       "\n",
       "### Background\n",
       "The FIFA World Cup 2002 was an international football tournament held from May 31 to June 30, 2002. Co-hosted by South Korea and Japan, it marked the first time that the event was staged in Asia and also the first time two countries jointly hosted the tournament. The competition featured 32 national teams, with France entering as the defending champions.\n",
       "\n",
       "### Significance\n",
       "The World Cup 2002 is remembered for several reasons. It was a landmark event in Asian football history, showcasing the continent's growing influence in the sport. The co-hosting arrangement set a precedent for future tournaments and demonstrated the potential of collaborative efforts in organizing major sporting events. Additionally, the tournament produced memorable matches and performances that continue to be celebrated by fans worldwide."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Host Countries\n",
       "\n",
       "### Selection Process\n",
       "The decision to co-host the 19th FIFA World Cup between South Korea and Japan was a groundbreaking move. The joint bid was chosen over Morocco in May 1996, marking the first time that two countries would host the tournament together. This selection process highlighted FIFA's desire to expand the global reach of the World Cup.\n",
       "\n",
       "### Preparations\n",
       "The preparations for the World Cup 2002 were extensive and involved significant infrastructure development. Both South Korea and Japan invested heavily in improving their stadiums, transportation networks, and accommodation facilities. The co-hosting nations worked closely with FIFA to ensure that all logistical aspects were meticulously planned and executed.\n",
       "\n",
       "### Stadiums\n",
       "The tournament featured a total of 16 stadiums across the two host countries. Each venue was carefully selected to provide optimal conditions for both players and spectators. Notable stadiums included the Seoul World Cup Stadium in South Korea, which hosted the opening match, and the International Stadium Yokohama in Japan, where the final took place."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Qualification Process\n",
       "\n",
       "### Regional Qualifiers\n",
       "The qualification process for the World Cup 2002 was a lengthy and competitive journey that spanned over two years. A total of 198 teams from FIFA's six confederations participated in regional qualifiers to secure one of the 32 spots available for the tournament.\n",
       "\n",
       "- **UEFA (Europe)**: 51 teams competed, with 13 qualifying for the World Cup.\n",
       "- **CAF (Africa)**: 51 teams competed, with 5 qualifying for the World Cup.\n",
       "- **CONMEBOL (South America)**: All 10 teams qualified automatically or through a playoff.\n",
       "- **CONCACAF (North and Central America and Caribbean)**: 35 teams competed, with 3.5 spots available (4 teams in total).\n",
       "- **AFC (Asia)**: 46 teams competed, with 4.5 spots available (5 teams in total).\n",
       "- **OFC (Oceania)**: 10 teams competed, with the winner advancing to an inter-confederation playoff.\n",
       "\n",
       "### Notable Absentees\n",
       "Several notable teams failed to qualify for the World Cup 2002, including:\n",
       "\n",
       "- France: The defending champions were eliminated in the playoffs by Denmark.\n",
       "- Portugal: Despite having a strong squad, they finished third in their qualifying group behind Ukraine and Poland.\n",
       "- Scotland: They narrowly missed out on qualification after losing a playoff to Norway."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Group Stage\n",
       "\n",
       "### Format\n",
       "The World Cup 2002 was structured with a group stage followed by knockout rounds. The 32 teams were divided into eight groups of four. Each team played against every other team in their group once, earning three points for a win, one point for a draw, and no points for a loss. The top two teams from each group advanced to the Round of 16.\n",
       "\n",
       "### Key Matches\n",
       "Several matches stood out during the group stage:\n",
       "- **South Korea vs. Poland**: This match was crucial as it determined which team would advance alongside the United States.\n",
       "- **Brazil vs. Turkey**: Brazil's dominant performance showcased their strength, while Turkey's resilience set the stage for future upsets.\n",
       "- **Germany vs. Saudi Arabia**: Germany's convincing win highlighted their early dominance in the tournament.\n",
       "\n",
       "### Surprises\n",
       "The group stage was not without its surprises:\n",
       "- **France's Early Exit**: The defending champions failed to advance beyond the group stage, a shocking outcome given their previous success.\n",
       "- **Senegal's Performance**: As one of the African teams, Senegal's strong showing against France and eventual qualification for the knockout stages was a significant surprise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Knockout Phase\n",
       "\n",
       "### Round of 16\n",
       "The Round of 16 marked the beginning of the knockout stage, where the top teams from each group faced off in single-elimination matches. Notable games included Germany's dramatic win over Paraguay and South Korea's upset victory over Italy. These matches set the stage for an exciting journey towards the final.\n",
       "\n",
       "### Quarterfinals\n",
       "In the Quarterfinals, the competition intensified with thrilling encounters. Brazil secured their spot in the Semifinals with a convincing win over England, while Turkey stunned the world with a penalty shootout victory against Japan. The other two matches saw South Korea continue their Cinderella run by defeating Spain, and Germany advancing after a hard-fought battle against the United States.\n",
       "\n",
       "### Semifinals\n",
       "The Semifinals featured some of the most memorable moments of the tournament. Brazil faced Turkey in a match that ended in a 1-0 victory for the Brazilians, securing their place in the final. Meanwhile, South Korea's remarkable journey came to an end as they lost to Germany in a closely contested match."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Final Match\n",
       "\n",
       "### Build-up\n",
       "The road to the 2002 World Cup final was filled with unexpected twists and turns. The tournament saw traditional powerhouses like Argentina and France eliminated in the group stages, making way for underdogs and surprise contenders. Brazil, led by the formidable Ronaldo, had a dominant run through the knockout stages, defeating teams like Belgium, England, and Turkey en route to the final. On the other side of the bracket, Germany, despite not being at their best, managed to secure victories against Paraguay, the United States, and South Korea to reach the final.\n",
       "\n",
       "### Match Details\n",
       "The final match between Brazil and Germany took place on June 30, 2002, at the International Stadium Yokohama in Japan. The atmosphere was electric, with fans from both nations filling the stands. The match started with both teams playing cautiously, but it was Brazil who struck first. Ronaldo scored the opening goal in the 67th minute, sending the Brazilian fans into raptures. Just three minutes later, Ronaldo added another goal, effectively sealing the victory for Brazil. Germany managed to pull one back through a penalty kick by Klose in the 90th minute, but it was too little, too late.\n",
       "\n",
       "### Aftermath\n",
       "Brazil's 2-0 victory over Germany made them the first team to win four World Cup titles. The triumph was particularly sweet for Ronaldo, who had famously suffered an epileptic seizure before the 1998 final and had been written off by many. His performance in the 2002 final not only silenced his critics but also cemented his legacy as one of the greatest footballers of all time. The 2002 World Cup final will be remembered for its dramatic build-up, thrilling match details, and the historic aftermath that followed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Key Players and Moments\n",
       "\n",
       "### Golden Boot Winner\n",
       "The Golden Boot award is given to the player who scores the most goals in a World Cup tournament. In 2002, this prestigious award was won by Ronaldo of Brazil. The legendary striker scored eight goals throughout the tournament, including two in the final against Germany. His performance was instrumental in Brazil's victory and solidified his status as one of the greatest footballers of all time.\n",
       "\n",
       "### Golden Ball Winner\n",
       "The Golden Ball is awarded to the best player of the World Cup. In 2002, this honor went to Oliver Kahn of Germany. Despite his team finishing as runners-up, Kahn's exceptional performances in goal were crucial for Germany's run to the final. His saves and leadership on the field made him a standout player of the tournament.\n",
       "\n",
       "### Memorable Goals\n",
       "The World Cup 2002 was filled with memorable goals that will be remembered by football fans for years to come. One such goal was scored by Michael Ballack in Germany's quarter-final match against the United States. His powerful strike from outside the box secured a 1-0 victory for Germany and showcased his incredible skill.\n",
       "\n",
       "Another unforgettable moment came in the final when Ronaldo scored two goals to lead Brazil to victory over Germany. His first goal was a brilliant individual effort, while his second was a clinical finish that sealed the win for the Brazilian team."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Legacy and Impact\n",
       "\n",
       "### Technical Innovations\n",
       "The World Cup 2002 introduced several technical innovations that have since become standard in international football tournaments. One of the most notable advancements was the use of advanced goal-line technology, which helped referees make more accurate decisions during matches. Additionally, the tournament saw the implementation of high-definition broadcasting, enhancing the viewing experience for fans worldwide.\n",
       "\n",
       "### Cultural Influence\n",
       "The World Cup 2002 had a profound cultural impact on both participating nations and global audiences. The event fostered a sense of unity and pride among the host countries, South Korea and Japan, as they successfully co-hosted the tournament. This collaboration set a precedent for future multi-nation hosting arrangements. Moreover, the tournament showcased diverse cultures through music, dance, and culinary experiences, enriching the global football community.\n",
       "\n",
       "### Future Tournaments\n",
       "The success of the World Cup 2002 influenced the planning and execution of subsequent tournaments. The lessons learned from this event, particularly in terms of organization, technology, and cultural integration, have been applied to improve future World Cups. For instance, the use of advanced technologies and the emphasis on cultural exchange have become integral parts of modern football tournaments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for s in article.sections: display(Markdown(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf07e3",
   "metadata": {},
   "source": [
    "# Getting Started III: Optimizing the LM prompts or weights in DSPy programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13492d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20b00683bcd467192c580589f8d6f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc4204e08b84fd5ab6b6b3fdd46d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hotpot_qa.py:   0%|          | 0.00/6.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4839241379a1461c808a5175abdf6f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fdb5e9f006400fbedc68d8a61a67eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11055020801d43938ad2e3332dbd363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7655b972fb493c9ab98d416de68ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ffdd61a5cd4b0097410bd4d94ebf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a711068aad436f9ce9cbb21a9702dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 18:31:35 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 20\n",
      "minibatch: True\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mProjected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\u001b[93m- Prompt Generation: \u001b[94m\u001b[1m10\u001b[0m\u001b[93m data summarizer calls + \u001b[94m\u001b[1m3\u001b[0m\u001b[93m * \u001b[94m\u001b[1m2\u001b[0m\u001b[93m lm calls in program + (\u001b[94m\u001b[1m3\u001b[0m\u001b[93m) lm calls in program-aware proposer = \u001b[94m\u001b[1m19\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\u001b[93m- Program Evaluation: \u001b[94m\u001b[1m35\u001b[0m\u001b[93m examples in minibatch * \u001b[94m\u001b[1m20\u001b[0m\u001b[93m batches + \u001b[94m\u001b[1m100\u001b[0m\u001b[93m examples in val set * \u001b[94m\u001b[1m5\u001b[0m\u001b[93m full evals = \u001b[94m\u001b[1m1200\u001b[0m\u001b[93m LM Program calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token)\n",
      "            + (Number of program calls * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the valset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
      "\u001b[93m- Setting `minibatch=True` if you haven't already.\u001b[0m\n",
      "\n",
      "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
      "If no input is received within 20 seconds, the program will proceed automatically.\n",
      "\n",
      "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`\u001b[0m when calling compile.\n",
      "\n",
      "\u001b[93mAwaiting your input...\u001b[0m\n",
      "\n",
      "Do you wish to continue? (y/n): "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 18:31:55 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/05/25 18:31:55 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/05/25 18:31:55 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No input received within 20 seconds. Proceeding with execution...\n",
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 11/100 [08:06<1:05:33, 44.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 11 examples for up to 1 rounds, amounting to 11 attempts.\n",
      "Bootstrapping set 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 7/100 [04:36<1:01:16, 39.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples for up to 1 rounds, amounting to 7 attempts.\n",
      "Bootstrapping set 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 5/100 [02:47<52:53, 33.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Bootstrapping set 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:12<20:20, 12.33s/it]\n",
      "2025/05/25 18:47:37 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/05/25 18:47:37 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 18:52:48 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n",
      "2025/05/25 18:58:33 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/05/25 19:03:41 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/05/25 19:05:52 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/05/25 19:07:02 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\n",
      "Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n",
      "\n",
      "To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
      "After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
      "\n",
      "When writing next_thought, you may reason about the current situation and plan for future steps.\n",
      "When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
      "\n",
      "(1) search_wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format.\n",
      "(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\n",
      "Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n",
      "\n",
      "To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
      "After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
      "\n",
      "When writing next_thought, you may reason about the current situation and plan for future steps.\n",
      "When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
      "\n",
      "(1) search_wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format.\n",
      "(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e., `answer`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: 2: {'question': \"Given the fields `question`, produce the fields `answer`.\\n\\nYou are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\\nYour goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\\n\\nTo do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\\nAfter each tool call, you receive a resulting observation, which gets appended to your trajectory.\\n\\nWhen writing next_thought, you may reason about the current situation and plan for future steps.\\nWhen selecting the next_tool_name and its next_tool_args, the tool must be one of:\\n(1) search_wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format.\\n(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\"}\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 1:\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: 1: {'fields': ['question'], 'response_fields': ['answer']}\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: 2: {'question': \"Are Truckin' Magazine and Girlfriends both publications that offer relationship advice?\", 'answer': 'No'}\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/05/25 19:07:32 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 25 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.00 / 100 (38.0%): : 102it [1:01:59, 36.46s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 20:09:31 INFO dspy.evaluate.evaluate: Average Metric: 38 / 100 (38.0%)\n",
      "2025/05/25 20:09:31 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 38.0\n",
      "\n",
      "/Users/michael/Library/CloudStorage/OneDrive-BGU/BGU/courses/anlp2025/dspy/.venv/lib/python3.11/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/05/25 20:09:31 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 13.00 / 35 (37.1%): 100%|| 35/35 [21:59<00:00, 37.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 20:31:30 INFO dspy.evaluate.evaluate: Average Metric: 13 / 35 (37.1%)\n",
      "2025/05/25 20:31:30 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 37.14 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 3', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 0'].\n",
      "2025/05/25 20:31:30 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14]\n",
      "2025/05/25 20:31:30 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0]\n",
      "2025/05/25 20:31:30 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 38.0\n",
      "2025/05/25 20:31:30 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/25 20:31:30 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 16.00 / 35 (45.7%): : 37it [30:51, 50.03s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 21:02:21 INFO dspy.evaluate.evaluate: Average Metric: 16 / 35 (45.7%)\n",
      "2025/05/25 21:02:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 45.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 5', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 2'].\n",
      "2025/05/25 21:02:21 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71]\n",
      "2025/05/25 21:02:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0]\n",
      "2025/05/25 21:02:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 38.0\n",
      "2025/05/25 21:02:21 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/25 21:02:21 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 14.00 / 35 (40.0%): : 37it [25:46, 41.79s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 21:28:07 INFO dspy.evaluate.evaluate: Average Metric: 14 / 35 (40.0%)\n",
      "2025/05/25 21:28:07 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 40.0 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 5', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 0'].\n",
      "2025/05/25 21:28:07 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0]\n",
      "2025/05/25 21:28:07 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0]\n",
      "2025/05/25 21:28:07 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 38.0\n",
      "2025/05/25 21:28:07 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/25 21:28:07 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.00 / 35 (54.3%): 100%|| 35/35 [27:48<00:00, 47.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 21:55:56 INFO dspy.evaluate.evaluate: Average Metric: 19 / 35 (54.3%)\n",
      "2025/05/25 21:55:56 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 54.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5', 'Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 4'].\n",
      "2025/05/25 21:55:56 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29]\n",
      "2025/05/25 21:55:56 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0]\n",
      "2025/05/25 21:55:56 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 38.0\n",
      "2025/05/25 21:55:56 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/25 21:55:56 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 21.00 / 35 (60.0%): : 37it [27:42, 44.93s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 22:23:38 INFO dspy.evaluate.evaluate: Average Metric: 21 / 35 (60.0%)\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.0 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 2'].\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0]\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0]\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 38.0\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 25 - Full Evaluation =====\n",
      "2025/05/25 22:23:38 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 60.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 55.00 / 100 (55.0%): : 102it [44:50, 26.38s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 23:08:29 INFO dspy.evaluate.evaluate: Average Metric: 55 / 100 (55.0%)\n",
      "2025/05/25 23:08:29 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 55.0\n",
      "2025/05/25 23:08:29 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0]\n",
      "2025/05/25 23:08:29 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/25 23:08:29 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/05/25 23:08:29 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/25 23:08:29 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 8 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 12.00 / 35 (34.3%): : 37it [17:55, 29.07s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 23:26:25 INFO dspy.evaluate.evaluate: Average Metric: 12 / 35 (34.3%)\n",
      "2025/05/25 23:26:25 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 34.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 5', 'Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 0'].\n",
      "2025/05/25 23:26:25 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29]\n",
      "2025/05/25 23:26:25 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0]\n",
      "2025/05/25 23:26:25 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/25 23:26:25 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/25 23:26:25 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 9 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 15.00 / 35 (42.9%): : 37it [36:36, 59.38s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 00:03:02 INFO dspy.evaluate.evaluate: Average Metric: 15 / 35 (42.9%)\n",
      "2025/05/26 00:03:02 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 42.86 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 2', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 1'].\n",
      "2025/05/26 00:03:02 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86]\n",
      "2025/05/26 00:03:02 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0]\n",
      "2025/05/26 00:03:02 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 00:03:02 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/05/26 00:03:02 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 10 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 12.00 / 35 (34.3%): 100%|| 35/35 [17:56<00:00, 30.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 00:20:58 INFO dspy.evaluate.evaluate: Average Metric: 12 / 35 (34.3%)\n",
      "2025/05/26 00:20:58 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 34.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 0', 'Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 0'].\n",
      "2025/05/26 00:20:58 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29]\n",
      "2025/05/26 00:20:58 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0]\n",
      "2025/05/26 00:20:59 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 00:20:59 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 00:20:59 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 11 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 16.00 / 35 (45.7%): 100%|| 35/35 [22:46<00:00, 39.05s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 00:43:45 INFO dspy.evaluate.evaluate: Average Metric: 16 / 35 (45.7%)\n",
      "2025/05/26 00:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 45.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 1', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 3'].\n",
      "2025/05/26 00:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71]\n",
      "2025/05/26 00:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0]\n",
      "2025/05/26 00:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 00:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 00:43:45 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 12 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 18.00 / 35 (51.4%): 100%|| 35/35 [00:00<00:00, 3587.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 00:43:46 INFO dspy.evaluate.evaluate: Average Metric: 18 / 35 (51.4%)\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 51.43 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 2'].\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43]\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0]\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 25 - Full Evaluation =====\n",
      "2025/05/26 00:43:46 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 54.29) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 54.00 / 100 (54.0%): 100%|| 100/100 [12:59<00:00,  7.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 00:56:46 INFO dspy.evaluate.evaluate: Average Metric: 54 / 100 (54.0%)\n",
      "2025/05/26 00:56:46 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0]\n",
      "2025/05/26 00:56:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 00:56:46 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/05/26 00:56:46 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/26 00:56:46 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 14 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.00 / 35 (54.3%): : 38it [45:35, 71.99s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 01:42:21 INFO dspy.evaluate.evaluate: Average Metric: 19 / 35 (54.3%)\n",
      "2025/05/26 01:42:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 54.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 2', 'Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 2'].\n",
      "2025/05/26 01:42:21 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29]\n",
      "2025/05/26 01:42:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0]\n",
      "2025/05/26 01:42:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 01:42:21 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 01:42:21 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 15 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 24.00 / 35 (68.6%): 100%|| 35/35 [29:07<00:00, 49.92s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 02:11:29 INFO dspy.evaluate.evaluate: Average Metric: 24 / 35 (68.6%)\n",
      "2025/05/26 02:11:29 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 68.57 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 5'].\n",
      "2025/05/26 02:11:29 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57]\n",
      "2025/05/26 02:11:29 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0]\n",
      "2025/05/26 02:11:29 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 02:11:29 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 02:11:29 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 16 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 18.00 / 35 (51.4%): : 36it [21:11, 35.32s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 02:32:40 INFO dspy.evaluate.evaluate: Average Metric: 18 / 35 (51.4%)\n",
      "2025/05/26 02:32:40 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 51.43 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 5'].\n",
      "2025/05/26 02:32:40 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43]\n",
      "2025/05/26 02:32:40 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0]\n",
      "2025/05/26 02:32:40 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 02:32:40 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 02:32:40 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 17 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 15.00 / 35 (42.9%): : 37it [18:58, 30.78s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 02:51:39 INFO dspy.evaluate.evaluate: Average Metric: 15 / 35 (42.9%)\n",
      "2025/05/26 02:51:39 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 42.86 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 5'].\n",
      "2025/05/26 02:51:39 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86]\n",
      "2025/05/26 02:51:39 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0]\n",
      "2025/05/26 02:51:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 02:51:39 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 02:51:39 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 18 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 21.00 / 35 (60.0%): : 38it [24:15, 38.30s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 03:15:54 INFO dspy.evaluate.evaluate: Average Metric: 21 / 35 (60.0%)\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.0 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 4'].\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86, 60.0]\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0]\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 19 / 25 - Full Evaluation =====\n",
      "2025/05/26 03:15:54 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 68.57) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 53.00 / 100 (53.0%): : 103it [35:36, 20.75s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 03:51:31 INFO dspy.evaluate.evaluate: Average Metric: 53 / 100 (53.0%)\n",
      "2025/05/26 03:51:31 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0]\n",
      "2025/05/26 03:51:31 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 03:51:31 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/05/26 03:51:31 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/26 03:51:31 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 20 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.00 / 35 (54.3%): 100%|| 35/35 [00:00<00:00, 4063.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 03:51:32 INFO dspy.evaluate.evaluate: Average Metric: 19 / 35 (54.3%)\n",
      "2025/05/26 03:51:32 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 54.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 5'].\n",
      "2025/05/26 03:51:32 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86, 60.0, 54.29]\n",
      "2025/05/26 03:51:32 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0]\n",
      "2025/05/26 03:51:32 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 03:51:32 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 03:51:32 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 21 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 21.00 / 35 (60.0%): : 37it [37:32, 60.88s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 04:29:04 INFO dspy.evaluate.evaluate: Average Metric: 21 / 35 (60.0%)\n",
      "2025/05/26 04:29:04 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.0 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 2'].\n",
      "2025/05/26 04:29:04 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86, 60.0, 54.29, 60.0]\n",
      "2025/05/26 04:29:04 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0]\n",
      "2025/05/26 04:29:04 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 04:29:04 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 04:29:04 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 22 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 18.00 / 35 (51.4%): 100%|| 35/35 [06:25<00:00, 11.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 04:35:30 INFO dspy.evaluate.evaluate: Average Metric: 18 / 35 (51.4%)\n",
      "2025/05/26 04:35:30 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 51.43 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 4'].\n",
      "2025/05/26 04:35:30 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86, 60.0, 54.29, 60.0, 51.43]\n",
      "2025/05/26 04:35:30 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0]\n",
      "2025/05/26 04:35:30 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 04:35:30 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 04:35:30 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 23 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 12.00 / 35 (34.3%): : 37it [24:47, 40.21s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 05:00:18 INFO dspy.evaluate.evaluate: Average Metric: 12 / 35 (34.3%)\n",
      "2025/05/26 05:00:18 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 34.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 3', 'Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 1'].\n",
      "2025/05/26 05:00:18 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86, 60.0, 54.29, 60.0, 51.43, 34.29]\n",
      "2025/05/26 05:00:18 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0]\n",
      "2025/05/26 05:00:18 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 05:00:18 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 05:00:18 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 24 / 25 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 20.00 / 35 (57.1%): : 37it [26:46, 43.42s/it]                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 05:27:05 INFO dspy.evaluate.evaluate: Average Metric: 20 / 35 (57.1%)\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.14 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 4', 'Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 5'].\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [37.14, 45.71, 40.0, 54.29, 60.0, 34.29, 42.86, 34.29, 45.71, 51.43, 54.29, 68.57, 51.43, 42.86, 60.0, 54.29, 60.0, 51.43, 34.29, 57.14]\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0]\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 25 / 25 - Full Evaluation =====\n",
      "2025/05/26 05:27:05 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 60.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 53.00 / 100 (53.0%): : 102it [1:00:52, 35.81s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 06:27:57 INFO dspy.evaluate.evaluate: Average Metric: 53 / 100 (53.0%)\n",
      "2025/05/26 06:27:57 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [38.0, 55.0, 54.0, 53.0, 53.0]\n",
      "2025/05/26 06:27:57 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.0\n",
      "2025/05/26 06:27:57 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/05/26 06:27:57 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/26 06:27:57 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 55.0!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimizing prompts for a ReAct agent\n",
    "import dspy\n",
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "# dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))\n",
    "\n",
    "def search_wikipedia(query: str) -> list[str]:\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train]\n",
    "react = dspy.ReAct(\"question -> answer\", tools=[search_wikipedia])\n",
    "\n",
    "tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto=\"light\", num_threads=24)\n",
    "optimized_react = tp.compile(react, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f80b3df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "react = Predict(StringSignature(question, trajectory -> next_thought, next_tool_name, next_tool_args\n",
       "    instructions='{\\'question\\': \"Given the fields `question`, produce the fields `answer`.\\\\n\\\\nYou are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\\\\nYour goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\\\\n\\\\nTo do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\\\\nAfter each tool call, you receive a resulting observation, which gets appended to your trajectory.\\\\n\\\\nWhen writing next_thought, you may reason about the current situation and plan for future steps.\\\\nWhen selecting the next_tool_name and its next_tool_args, the tool must be one of:\\\\n(1) search_wikipedia. It takes arguments {\\'query\\': {\\'type\\': \\'string\\'}} in JSON format.\\\\n(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\"}'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    trajectory = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Trajectory:', 'desc': '${trajectory}'})\n",
       "    next_thought = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Next Thought:', 'desc': '${next_thought}'})\n",
       "    next_tool_name = Field(annotation=Literal['search_wikipedia', 'finish'] required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Next Tool Name:', 'desc': '${next_tool_name}'})\n",
       "    next_tool_args = Field(annotation=dict[str, Any] required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Next Tool Args:', 'desc': '${next_tool_args}'})\n",
       "))\n",
       "extract.predict = Predict(StringSignature(question, trajectory -> reasoning, answer\n",
       "    instructions='{\\'question\\': \"Are Truckin\\' Magazine and Girlfriends both publications that offer relationship advice?\", \\'answer\\': \\'No\\'}'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    trajectory = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Trajectory:', 'desc': '${trajectory}'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       "))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_react"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98977d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - hw2)",
   "language": "python",
   "name": "hw2-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
